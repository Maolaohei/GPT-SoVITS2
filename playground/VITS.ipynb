{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前目录: /workspaces/GPT-SoVITS2/playground\n",
      "/workspaces/GPT-SoVITS2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(\"当前目录:\", current_dir)\n",
    "# 获取当前文件的父目录\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "print(parent_dir)\n",
    "from GPT_SoVITS.module.models import SynthesizerTrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SynthesizerTrn(\n",
      "  (enc_p): TextEncoder(\n",
      "    (ssl_proj): Conv1d(768, 384, kernel_size=(1,), stride=(1,))\n",
      "    (encoder_ssl): Encoder(\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (attn_layers): ModuleList(\n",
      "        (0-2): 3 x MultiHeadAttention(\n",
      "          (conv_q): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "          (conv_k): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "          (conv_v): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "          (conv_o): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm_layers_1): ModuleList(\n",
      "        (0-2): 3 x LayerNorm()\n",
      "      )\n",
      "      (ffn_layers): ModuleList(\n",
      "        (0-2): 3 x FFN(\n",
      "          (conv_1): Conv1d(384, 768, kernel_size=(3,), stride=(1,))\n",
      "          (conv_2): Conv1d(768, 384, kernel_size=(3,), stride=(1,))\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm_layers_2): ModuleList(\n",
      "        (0-2): 3 x LayerNorm()\n",
      "      )\n",
      "      (spk_emb_linear): Linear(in_features=384, out_features=384, bias=True)\n",
      "    )\n",
      "    (proj): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (dec): Generator(\n",
      "    (conv_pre): Conv1d(384, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "    (ups): ModuleList(\n",
      "      (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(10,), padding=(3,))\n",
      "      (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "      (2): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(2,), padding=(3,))\n",
      "      (3): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))\n",
      "      (4): ConvTranspose1d(32, 16, kernel_size=(2,), stride=(2,))\n",
      "    )\n",
      "    (resblocks): ModuleList(\n",
      "      (0): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (1): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "          (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        )\n",
      "      )\n",
      "      (2): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "          (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "          (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        )\n",
      "      )\n",
      "      (3): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (4): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "          (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        )\n",
      "      )\n",
      "      (5): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "          (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "          (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        )\n",
      "      )\n",
      "      (6): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (7): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "          (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        )\n",
      "      )\n",
      "      (8): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "          (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "          (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        )\n",
      "      )\n",
      "      (9): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (10): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "          (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        )\n",
      "      )\n",
      "      (11): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "          (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "          (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        )\n",
      "      )\n",
      "      (12): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "          (2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (13): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "          (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        )\n",
      "      )\n",
      "      (14): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "          (1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "          (2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_post): Conv1d(16, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "    (cond): Conv1d(384, 512, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (enc_q): PosteriorEncoder(\n",
      "    (pre): Conv1d(1025, 384, kernel_size=(1,), stride=(1,))\n",
      "    (enc): WN(\n",
      "      (in_layers): ModuleList(\n",
      "        (0-15): 16 x Conv1d(384, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "      (res_skip_layers): ModuleList(\n",
      "        (0-14): 15 x Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "        (15): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (drop): Dropout(p=0, inplace=False)\n",
      "      (cond_layer): Conv1d(384, 12288, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (proj): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (flow): ResidualCouplingTransformersBlock(\n",
      "    (flows): ModuleList(\n",
      "      (0): FFTransformerCouplingLayer(\n",
      "        (pre): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "        (enc): FFT(\n",
      "          (cond_pre): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "          (cond_layer): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0, inplace=False)\n",
      "          (self_attn_layers): ModuleList(\n",
      "            (0): MultiHeadAttention(\n",
      "              (conv_q): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_k): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_v): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_o): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_0): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "          (ffn_layers): ModuleList(\n",
      "            (0): FFN(\n",
      "              (conv_1): Conv1d(384, 768, kernel_size=(5,), stride=(1,))\n",
      "              (conv_2): Conv1d(768, 384, kernel_size=(5,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_1): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (post): Conv1d(384, 192, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (1): Flip()\n",
      "      (2): FFTransformerCouplingLayer(\n",
      "        (pre): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "        (enc): FFT(\n",
      "          (cond_pre): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "          (cond_layer): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0, inplace=False)\n",
      "          (self_attn_layers): ModuleList(\n",
      "            (0): MultiHeadAttention(\n",
      "              (conv_q): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_k): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_v): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_o): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_0): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "          (ffn_layers): ModuleList(\n",
      "            (0): FFN(\n",
      "              (conv_1): Conv1d(384, 768, kernel_size=(5,), stride=(1,))\n",
      "              (conv_2): Conv1d(768, 384, kernel_size=(5,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_1): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (post): Conv1d(384, 192, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (3): Flip()\n",
      "      (4): FFTransformerCouplingLayer(\n",
      "        (pre): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "        (enc): FFT(\n",
      "          (cond_pre): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "          (cond_layer): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0, inplace=False)\n",
      "          (self_attn_layers): ModuleList(\n",
      "            (0): MultiHeadAttention(\n",
      "              (conv_q): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_k): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_v): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_o): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_0): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "          (ffn_layers): ModuleList(\n",
      "            (0): FFN(\n",
      "              (conv_1): Conv1d(384, 768, kernel_size=(5,), stride=(1,))\n",
      "              (conv_2): Conv1d(768, 384, kernel_size=(5,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_1): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (post): Conv1d(384, 192, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (5): Flip()\n",
      "      (6): FFTransformerCouplingLayer(\n",
      "        (pre): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "        (enc): FFT(\n",
      "          (cond_pre): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "          (cond_layer): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0, inplace=False)\n",
      "          (self_attn_layers): ModuleList(\n",
      "            (0): MultiHeadAttention(\n",
      "              (conv_q): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_k): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_v): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (conv_o): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_0): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "          (ffn_layers): ModuleList(\n",
      "            (0): FFN(\n",
      "              (conv_1): Conv1d(384, 768, kernel_size=(5,), stride=(1,))\n",
      "              (conv_2): Conv1d(768, 384, kernel_size=(5,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_1): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (post): Conv1d(384, 192, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (7): Flip()\n",
      "    )\n",
      "  )\n",
      "  (ref_enc): MelStyleEncoder(\n",
      "    (spectral): Sequential(\n",
      "      (0): LinearNorm(\n",
      "        (fc): Linear(in_features=1025, out_features=128, bias=True)\n",
      "      )\n",
      "      (1): Mish()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): LinearNorm(\n",
      "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (4): Mish()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (temporal): Sequential(\n",
      "      (0): Conv1dGLU(\n",
      "        (conv1): ConvNorm(\n",
      "          (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): Conv1dGLU(\n",
      "        (conv1): ConvNorm(\n",
      "          (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (slf_attn): MultiHeadAttention(\n",
      "      (w_qs): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (w_ks): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (w_vs): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (attention): ScaledDotProductAttention(\n",
      "        (softmax): Softmax(dim=2)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (fc): LinearNorm(\n",
      "      (fc): Linear(in_features=128, out_features=384, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ssl_proj): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "  (quantizer): ResidualVectorQuantizer(\n",
      "    (vq): ResidualVectorQuantization(\n",
      "      (layers): ModuleList(\n",
      "        (0): VectorQuantization(\n",
      "          (project_in): Identity()\n",
      "          (project_out): Identity()\n",
      "          (_codebook): EuclideanCodebook()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "kmeans start ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0494,  0.0450, -0.0023,  ..., -0.0417, -0.0369, -0.0526]]],\n",
       "        grad_fn=<TanhBackward0>),\n",
       " tensor(0.),\n",
       " tensor([16]),\n",
       " tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]),\n",
       " tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]),\n",
       " (tensor([[[ 0.1181,  0.7924,  0.4886,  ..., -1.0518, -0.1100, -0.9764],\n",
       "           [-0.7224, -0.3955, -0.2678,  ...,  0.3018,  1.0651, -0.1570],\n",
       "           [-0.5043,  1.7385, -1.5613,  ..., -1.8755,  1.1939, -0.6386],\n",
       "           ...,\n",
       "           [ 0.8140, -1.1253, -0.2656,  ..., -1.6565,  1.4282, -1.6696],\n",
       "           [-0.2990, -0.6538, -0.7360,  ..., -0.5513,  0.2178, -0.0039],\n",
       "           [-0.1031, -1.3498, -0.5900,  ...,  1.1885,  1.2119,  0.6590]]],\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 0.1181,  0.7924,  0.4886,  ..., -1.0518, -0.1100, -0.9764],\n",
       "           [-0.7224, -0.3955, -0.2678,  ...,  0.3018,  1.0651, -0.1570],\n",
       "           [-0.5043,  1.7385, -1.5613,  ..., -1.8755,  1.1939, -0.6386],\n",
       "           ...,\n",
       "           [ 0.8140, -1.1253, -0.2656,  ..., -1.6565,  1.4282, -1.6696],\n",
       "           [-0.2990, -0.6538, -0.7360,  ..., -0.5513,  0.2178, -0.0039],\n",
       "           [-0.1031, -1.3498, -0.5900,  ...,  1.1885,  1.2119,  0.6590]]],\n",
       "         grad_fn=<FlipBackward0>),\n",
       "  tensor([[[ 0.7306, -0.6136, -0.5669,  ...,  0.0814, -0.0906, -0.6603],\n",
       "           [ 0.1465, -0.1695, -0.5917,  ...,  0.3941, -0.8666,  0.6003],\n",
       "           [ 0.3628, -0.5764,  0.1486,  ...,  0.1624,  0.5622, -0.2121],\n",
       "           ...,\n",
       "           [-0.6775, -0.5633,  0.4787,  ..., -1.0555, -0.8180,  0.0206],\n",
       "           [ 1.5719,  0.2906, -0.0969,  ..., -0.2750, -0.9454, -0.3306],\n",
       "           [ 0.0142, -0.9464, -1.0777,  ..., -0.5536, -0.2500, -0.1014]]],\n",
       "         grad_fn=<SplitBackward0>),\n",
       "  tensor([[[ 0.3166, -0.2428, -0.0333,  ..., -0.6134, -0.4590, -0.6044],\n",
       "           [ 0.1440,  0.7185,  0.0865,  ..., -1.1148,  0.4669, -0.3215],\n",
       "           [ 0.1288,  0.0095,  0.6232,  ...,  0.4813,  0.0572,  0.3897],\n",
       "           ...,\n",
       "           [ 0.4171,  0.4339,  0.0286,  ...,  0.0467,  0.2501,  0.0640],\n",
       "           [ 0.1258, -0.8633, -0.1984,  ..., -0.1829, -0.2111, -0.3344],\n",
       "           [ 0.1370,  0.3808, -0.5172,  ...,  0.6953, -0.3390,  0.2203]]],\n",
       "         grad_fn=<SplitBackward0>),\n",
       "  tensor([[[-0.1602,  0.1003,  0.1414,  ...,  0.2603,  0.1903,  0.4014],\n",
       "           [-0.0988, -0.0427, -0.0700,  ...,  0.1638,  0.0376,  0.0891],\n",
       "           [-0.0450,  0.2364, -0.0348,  ...,  0.3090,  0.0448,  0.2381],\n",
       "           ...,\n",
       "           [-0.1766, -0.1001,  0.2835,  ..., -0.2204, -0.3199,  0.0671],\n",
       "           [ 0.0146, -0.2276,  0.1611,  ...,  0.1870, -0.2152,  0.1013],\n",
       "           [ 0.0101, -0.0533,  0.0435,  ...,  0.2629, -0.0790, -0.0768]]],\n",
       "         grad_fn=<SplitBackward0>),\n",
       "  tensor([[[-0.0431,  0.1413, -0.1255,  ...,  0.2352,  0.1102,  0.1137],\n",
       "           [-0.1276,  0.0139, -0.1250,  ..., -0.5559,  0.0673, -0.0148],\n",
       "           [ 0.1220,  0.1037,  0.2339,  ...,  0.1655,  0.0406, -0.0221],\n",
       "           ...,\n",
       "           [-0.0290, -0.0145,  0.2130,  ..., -0.0759, -0.0860,  0.1419],\n",
       "           [ 0.0887,  0.0917, -0.1912,  ..., -0.1722,  0.0134, -0.1560],\n",
       "           [-0.0262,  0.0525, -0.1966,  ..., -0.1146, -0.1234, -0.1206]]],\n",
       "         grad_fn=<SplitBackward0>)),\n",
       " tensor([[[ 1.1340, -0.9710, -0.2972,  ...,  1.0722,  0.4375, -0.7393],\n",
       "          [ 0.7725, -0.6987, -0.1477,  ..., -0.3198, -0.7667, -1.3633],\n",
       "          [ 0.6078, -0.0710, -0.2346,  ..., -0.1094, -0.2529, -0.8971],\n",
       "          ...,\n",
       "          [ 1.0001, -0.2659,  0.5548,  ...,  0.4562, -0.1560,  0.3845],\n",
       "          [ 0.5076, -0.2558, -0.5375,  ...,  0.0621, -0.1711, -0.0481],\n",
       "          [-0.4302, -0.4815,  0.1625,  ...,  1.0302, -0.6245, -0.1271]]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    # hps.data.filter_length // 2 + 1,\n",
    "    #     hps.train.segment_size // hps.data.hop_length,\n",
    "    \"spec_channels\": 2048 // 2 + 1,\n",
    "    \"segment_size\": 20480 // 640,\n",
    "    \"inter_channels\": 384,\n",
    "    \"hidden_channels\": 384,\n",
    "    \"filter_channels\": 768,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_layers\": 6,\n",
    "    \"kernel_size\": 3,\n",
    "    \"p_dropout\": 0.1,\n",
    "    \"resblock\": \"1\",\n",
    "    \"resblock_kernel_sizes\": [3, 7, 11],\n",
    "    \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "    \"upsample_rates\": [10, 8, 2, 2, 2],\n",
    "    \"upsample_initial_channel\": 512,\n",
    "    \"upsample_kernel_sizes\": [16, 16, 8, 2, 2],\n",
    "    \"semantic_frame_rate\": \"50hz\",\n",
    "    \"freeze_quantizer\": True,\n",
    "}\n",
    "net_g = SynthesizerTrn(**params)\n",
    "print(net_g)\n",
    "import torch\n",
    "import time\n",
    "net_g.forward(\n",
    "    ssl=torch.randn(1, 768, 50),\n",
    "    y=torch.randn(1, 2048 // 2 + 1, 50),\n",
    "    y_lengths=torch.tensor([50]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.938814163208008\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for _ in range(5):\n",
    "\n",
    "    res = net_g.infer(\n",
    "        ssl=torch.randn(1, 768, 100),\n",
    "        y=torch.randn(1, 2048 // 2 + 1, 100),\n",
    "        y_lengths=torch.tensor([100]),\n",
    "    )\n",
    "    res = net_g.extract_latent(\n",
    "        x=torch.randn(1, 768, 100),\n",
    "    )\n",
    "    res = net_g.decode(\n",
    "        codes=res,\n",
    "        refer=torch.randn(1, 2048 // 2 + 1, 100),\n",
    "    )\n",
    "    res.shape\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
