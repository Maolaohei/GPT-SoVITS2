{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前目录: /workspaces/GPT-SoVITS2/playground\n",
      "/workspaces/GPT-SoVITS2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(\"当前目录:\", current_dir)\n",
    "# 获取当前文件的父目录\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "print(parent_dir)\n",
    "from GPT_SoVITS.module.models import SynthesizerTrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 124083, 6146, 3, 24474, 353, 49124, 6447, 32, 5, 55719, 59724, 15446, 93984]\n",
      "tensor([[     5, 124083,   6146,      3,  24474,    353,  49124,   6447,     32,\n",
      "              5,  55719,  59724,  15446,  93984]])\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"./sentencepiece.bpe.model\")\n",
    "token = sp.encode(\"你好啊，这里是测试token embedding效果怎么样\")\n",
    "print(token)\n",
    "# 增加batch size 维度\n",
    "token = torch.tensor(token).unsqueeze(0)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SynthesizerTrn(\n",
      "  (enc_p): TextEncoder(\n",
      "    (ssl_proj): Conv1d(768, 256, kernel_size=(1,), stride=(1,))\n",
      "    (encoder_ssl): Encoder(\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (attn_layers): ModuleList(\n",
      "        (0-5): 6 x MultiHeadAttention(\n",
      "          (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm_layers_1): ModuleList(\n",
      "        (0-5): 6 x LayerNorm()\n",
      "      )\n",
      "      (ffn_layers): ModuleList(\n",
      "        (0-5): 6 x FFN(\n",
      "          (conv_1): Conv1d(256, 1024, kernel_size=(3,), stride=(1,))\n",
      "          (conv_2): Conv1d(1024, 256, kernel_size=(3,), stride=(1,))\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm_layers_2): ModuleList(\n",
      "        (0-5): 6 x LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (encoder_text): Encoder(\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (attn_layers): ModuleList(\n",
      "        (0-5): 6 x MultiHeadAttention(\n",
      "          (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm_layers_1): ModuleList(\n",
      "        (0-5): 6 x LayerNorm()\n",
      "      )\n",
      "      (ffn_layers): ModuleList(\n",
      "        (0-5): 6 x FFN(\n",
      "          (conv_1): Conv1d(256, 1024, kernel_size=(3,), stride=(1,))\n",
      "          (conv_2): Conv1d(1024, 256, kernel_size=(3,), stride=(1,))\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm_layers_2): ModuleList(\n",
      "        (0-5): 6 x LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (text_embedding): Embedding(250000, 256)\n",
      "    (mrte): MRTE(\n",
      "      (cross_attention): MultiHeadAttention(\n",
      "        (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (dec): Generator(\n",
      "    (conv_pre): Conv1d(256, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "    (ups): ModuleList(\n",
      "      (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(10,), padding=(3,))\n",
      "      (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "      (2): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(2,), padding=(3,))\n",
      "      (3): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))\n",
      "      (4): ConvTranspose1d(32, 16, kernel_size=(2,), stride=(2,))\n",
      "    )\n",
      "    (resblocks): ModuleList(\n",
      "      (0): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (1): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "          (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        )\n",
      "      )\n",
      "      (2): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "          (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "          (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        )\n",
      "      )\n",
      "      (3): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (4): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "          (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        )\n",
      "      )\n",
      "      (5): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "          (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "          (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        )\n",
      "      )\n",
      "      (6): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (7): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "          (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        )\n",
      "      )\n",
      "      (8): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "          (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "          (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        )\n",
      "      )\n",
      "      (9): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (10): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "          (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        )\n",
      "      )\n",
      "      (11): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "          (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "          (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        )\n",
      "      )\n",
      "      (12): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "          (2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (13): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "          (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        )\n",
      "      )\n",
      "      (14): ResBlock1(\n",
      "        (convs1): ModuleList(\n",
      "          (0): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "          (1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "          (2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "        )\n",
      "        (convs2): ModuleList(\n",
      "          (0-2): 3 x Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_post): Conv1d(16, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "    (cond): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (enc_q): PosteriorEncoder(\n",
      "    (pre): Conv1d(1025, 256, kernel_size=(1,), stride=(1,))\n",
      "    (enc): WN(\n",
      "      (in_layers): ModuleList(\n",
      "        (0-15): 16 x Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "      (res_skip_layers): ModuleList(\n",
      "        (0-14): 15 x Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "        (15): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (drop): Dropout(p=0, inplace=False)\n",
      "      (cond_layer): Conv1d(256, 8192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (proj): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (flow): ResidualCouplingTransformersBlock(\n",
      "    (flows): ModuleList(\n",
      "      (0): FFTransformerCouplingLayer(\n",
      "        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "        (enc): FFT(\n",
      "          (cond_pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (cond_layer): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0, inplace=False)\n",
      "          (self_attn_layers): ModuleList(\n",
      "            (0): MultiHeadAttention(\n",
      "              (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_0): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "          (ffn_layers): ModuleList(\n",
      "            (0): FFN(\n",
      "              (conv_1): Conv1d(256, 768, kernel_size=(5,), stride=(1,))\n",
      "              (conv_2): Conv1d(768, 256, kernel_size=(5,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_1): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (post): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (1): Flip()\n",
      "      (2): FFTransformerCouplingLayer(\n",
      "        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "        (enc): FFT(\n",
      "          (cond_pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (cond_layer): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0, inplace=False)\n",
      "          (self_attn_layers): ModuleList(\n",
      "            (0): MultiHeadAttention(\n",
      "              (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_0): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "          (ffn_layers): ModuleList(\n",
      "            (0): FFN(\n",
      "              (conv_1): Conv1d(256, 768, kernel_size=(5,), stride=(1,))\n",
      "              (conv_2): Conv1d(768, 256, kernel_size=(5,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_1): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (post): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (3): Flip()\n",
      "      (4): FFTransformerCouplingLayer(\n",
      "        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "        (enc): FFT(\n",
      "          (cond_pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (cond_layer): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0, inplace=False)\n",
      "          (self_attn_layers): ModuleList(\n",
      "            (0): MultiHeadAttention(\n",
      "              (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_0): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "          (ffn_layers): ModuleList(\n",
      "            (0): FFN(\n",
      "              (conv_1): Conv1d(256, 768, kernel_size=(5,), stride=(1,))\n",
      "              (conv_2): Conv1d(768, 256, kernel_size=(5,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_1): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (post): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (5): Flip()\n",
      "      (6): FFTransformerCouplingLayer(\n",
      "        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "        (enc): FFT(\n",
      "          (cond_pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (cond_layer): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (drop): Dropout(p=0, inplace=False)\n",
      "          (self_attn_layers): ModuleList(\n",
      "            (0): MultiHeadAttention(\n",
      "              (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_0): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "          (ffn_layers): ModuleList(\n",
      "            (0): FFN(\n",
      "              (conv_1): Conv1d(256, 768, kernel_size=(5,), stride=(1,))\n",
      "              (conv_2): Conv1d(768, 256, kernel_size=(5,), stride=(1,))\n",
      "              (drop): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm_layers_1): ModuleList(\n",
      "            (0): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (post): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (7): Flip()\n",
      "    )\n",
      "  )\n",
      "  (ref_enc): MelStyleEncoder(\n",
      "    (spectral): Sequential(\n",
      "      (0): LinearNorm(\n",
      "        (fc): Linear(in_features=1025, out_features=128, bias=True)\n",
      "      )\n",
      "      (1): Mish()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): LinearNorm(\n",
      "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (4): Mish()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (temporal): Sequential(\n",
      "      (0): Conv1dGLU(\n",
      "        (conv1): ConvNorm(\n",
      "          (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): Conv1dGLU(\n",
      "        (conv1): ConvNorm(\n",
      "          (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (slf_attn): MultiHeadAttention(\n",
      "      (w_qs): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (w_ks): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (w_vs): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (attention): ScaledDotProductAttention(\n",
      "        (softmax): Softmax(dim=2)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (fc): LinearNorm(\n",
      "      (fc): Linear(in_features=128, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ssl_proj): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "  (quantizer): GroupedResidualFSQ(\n",
      "    (rvqs): ModuleList(\n",
      "      (0-1): 2 x ResidualFSQ(\n",
      "        (project_in): Linear(in_features=384, out_features=4, bias=True)\n",
      "        (project_out): Linear(in_features=4, out_features=384, bias=True)\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x FSQ(\n",
      "            (project_in): Identity()\n",
      "            (project_out): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 256, 100])\n",
      "b: 1\n",
      "ids_str_max tensor([69])\n",
      "ids_str tensor([33])\n",
      "torch.Size([1, 256, 32]) torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111154/3654712241.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  token=torch.tensor(token),\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "params = {\n",
    "    # hps.data.filter_length // 2 + 1,\n",
    "    #     hps.train.segment_size // hps.data.hop_length,\n",
    "    \"spec_channels\": 2048 // 2 + 1,\n",
    "    \"segment_size\": 20480 // 640,\n",
    "    \"inter_channels\": 256,\n",
    "    \"hidden_channels\": 256,\n",
    "    \"filter_channels\": 1024,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 6,\n",
    "    \"kernel_size\": 3,\n",
    "    \"p_dropout\": 0.1,\n",
    "    \"resblock\": \"1\",\n",
    "    \"resblock_kernel_sizes\": [3, 7, 11],\n",
    "    \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "    \"upsample_rates\": [10, 8, 2, 2, 2],\n",
    "    # \"upsample_rates\": [8,8,2,2],\n",
    "    \"upsample_initial_channel\": 512,\n",
    "    \"upsample_kernel_sizes\": [16, 16, 8, 2, 2],\n",
    "    # \"upsample_kernel_sizes\": [16,16,4,4],\n",
    "    \"semantic_frame_rate\": \"50hz\",\n",
    "    \"freeze_quantizer\": True,\n",
    "}\n",
    "net_g = SynthesizerTrn(**params)\n",
    "print(net_g)\n",
    "\n",
    "res = net_g.forward(\n",
    "    ssl=torch.randn(1, 768, 100),\n",
    "    y=torch.randn(1, 2048 // 2 + 1, 100),\n",
    "    y_lengths=torch.tensor([100]),\n",
    "    token=torch.tensor(token),\n",
    "    token_lengths=torch.tensor([token.shape[1]]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: 128489352\n"
     ]
    }
   ],
   "source": [
    "print(\"number of trainable parameters:\", sum(p.numel() for p in net_g.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104519/1100293123.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  token=torch.tensor(token),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41435980796813965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104519/1100293123.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  token=torch.tensor(token),\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for _ in range(1):\n",
    "    res = net_g.infer(\n",
    "        ssl=torch.randn(1, 768, 50),\n",
    "        y=torch.randn(1, 2048 // 2 + 1, 50),\n",
    "        y_lengths=torch.tensor([50]),\n",
    "        token=torch.tensor(token),\n",
    "        token_lengths=torch.tensor([token.shape[1]]),\n",
    "    )\n",
    "    res = net_g.extract_latent(\n",
    "        x=torch.randn(1, 768, 50),\n",
    "    )\n",
    "    res = net_g.decode(\n",
    "        codes=res.transpose(-1, -2),\n",
    "        token=torch.tensor(token),\n",
    "        refer=torch.randn(1, 2048 // 2 + 1, 50),\n",
    "    )\n",
    "    res.shape\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(net_g, \"net_g.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
