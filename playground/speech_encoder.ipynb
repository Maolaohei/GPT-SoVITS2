{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import librosa\n",
    "import torch\n",
    "max_val = 0.8\n",
    "target_sr = 16000\n",
    "def load_wav(wav, target_sr):\n",
    "    speech, sample_rate = torchaudio.load(wav)\n",
    "    speech = speech.mean(dim=0, keepdim=True)\n",
    "    if sample_rate != target_sr:\n",
    "        assert sample_rate > target_sr, 'wav sample rate {} must be greater than {}'.format(sample_rate, target_sr)\n",
    "        speech = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)(speech)\n",
    "    return speech\n",
    "def postprocess(speech, top_db=60, hop_length=220, win_length=440):\n",
    "    speech, _ = librosa.effects.trim(\n",
    "        speech, top_db=top_db,\n",
    "        frame_length=win_length,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    if speech.abs().max() > max_val:\n",
    "        speech = speech / speech.abs().max() * max_val\n",
    "    speech = torch.concat([speech, torch.zeros(1, int(target_sr * 0.2))], dim=1)\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "speech_16k = postprocess(load_wav(\"./1.mp3\", 16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语音时长: 6.8318 秒\n"
     ]
    }
   ],
   "source": [
    "# 计算语音时长\n",
    "audio_duration = speech_16k.shape[1] / target_sr\n",
    "\n",
    "print(f\"语音时长: {audio_duration:.4f} 秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = speech_16k\n",
    "import whisper\n",
    "# load \n",
    "feat = whisper.log_mel_spectrogram(feat, n_mels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-25 16:37:40.965221772 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 12 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-07-25 16:37:40.966285049 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-07-25 16:37:40.966290949 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "option = onnxruntime.SessionOptions()\n",
    "option.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "option.intra_op_num_threads = 1\n",
    "speech_tokenizer_session = onnxruntime.InferenceSession(\n",
    "    \"../pretrained_models/speech_tokenizer_v1.onnx\", sess_options=option, providers=[\"CUDAExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats\n",
      "feats_length\n"
     ]
    }
   ],
   "source": [
    "print(speech_tokenizer_session.get_inputs()[0].name)\n",
    "print(speech_tokenizer_session.get_inputs()[1].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.45053625, -0.05974329, -0.25894833, ..., -0.67959416,\n",
       "         -0.67959416, -0.67959416],\n",
       "        [-0.3529718 ,  0.03782111, -0.16138399, ..., -0.67959416,\n",
       "         -0.67959416, -0.67959416],\n",
       "        [-0.37882137,  0.08174139,  0.09368938, ..., -0.67959416,\n",
       "         -0.67959416, -0.67959416],\n",
       "        ...,\n",
       "        [-0.29448664,  0.06283569,  0.16168153, ..., -0.67959416,\n",
       "         -0.67959416, -0.67959416],\n",
       "        [-0.38084733, -0.13342845, -0.03013408, ..., -0.67959416,\n",
       "         -0.67959416, -0.67959416],\n",
       "        [-0.420272  , -0.16023862, -0.10880888, ..., -0.67959416,\n",
       "         -0.67959416, -0.67959416]]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_length = np.array([feat.shape[2]], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_length: [683]\n"
     ]
    }
   ],
   "source": [
    "print(f\"feat_length: {feat_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_token = speech_tokenizer_session.run(None, {speech_tokenizer_session.get_inputs()[0].name: feat.detach().cpu().numpy(),\n",
    "                                                                speech_tokenizer_session.get_inputs()[1].name: np.array([feat.shape[2]], dtype=np.int32)})[0].flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_token = torch.Tensor(speech_token).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前目录: /workspaces/GPT-SoVITS2/playground\n",
      "/workspaces/GPT-SoVITS2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(\"当前目录:\", current_dir)\n",
    "# 获取当前文件的父目录\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "print(parent_dir)\n",
    "from GPT_SoVITS.module.models import SynthesizerTrn\n",
    "import time\n",
    "\n",
    "params = {\n",
    "    \"spec_channels\": 2048 // 2 + 1,\n",
    "    \"segment_size\": 20480 // 640,\n",
    "    \"inter_channels\": 256,\n",
    "    \"hidden_channels\": 256,\n",
    "    \"filter_channels\": 1024,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 6,\n",
    "    \"kernel_size\": 3,\n",
    "    \"p_dropout\": 0.1,\n",
    "    \"resblock\": \"1\",\n",
    "    \"resblock_kernel_sizes\": [3, 7, 11],\n",
    "    \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "    \"upsample_rates\": [10, 8, 2, 2, 2],\n",
    "    \"upsample_initial_channel\": 512,\n",
    "    \"upsample_kernel_sizes\": [16, 16, 8, 2, 2],\n",
    "}\n",
    "net_g = SynthesizerTrn(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SynthesizerTrn(\n",
       "  (embedding): Embedding(4097, 256)\n",
       "  (rotary_emb): RotaryEmbedding()\n",
       "  (enc_p): TextEncoder(\n",
       "    (encoder_ssl): Encoder(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (attn_layers): ModuleList(\n",
       "        (0-5): 6 x MultiHeadAttention(\n",
       "          (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layers_1): ModuleList(\n",
       "        (0-5): 6 x LayerNorm()\n",
       "      )\n",
       "      (ffn_layers): ModuleList(\n",
       "        (0-5): 6 x FFN(\n",
       "          (conv_1): Conv1d(256, 1024, kernel_size=(3,), stride=(1,))\n",
       "          (conv_2): Conv1d(1024, 256, kernel_size=(3,), stride=(1,))\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layers_2): ModuleList(\n",
       "        (0-5): 6 x LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (encoder_text): Encoder(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (attn_layers): ModuleList(\n",
       "        (0-5): 6 x MultiHeadAttention(\n",
       "          (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layers_1): ModuleList(\n",
       "        (0-5): 6 x LayerNorm()\n",
       "      )\n",
       "      (ffn_layers): ModuleList(\n",
       "        (0-5): 6 x FFN(\n",
       "          (conv_1): Conv1d(256, 1024, kernel_size=(3,), stride=(1,))\n",
       "          (conv_2): Conv1d(1024, 256, kernel_size=(3,), stride=(1,))\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layers_2): ModuleList(\n",
       "        (0-5): 6 x LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (text_embedding): Embedding(250000, 256)\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (mrte): MRTE(\n",
       "      (cross_attention): MultiHeadAttention(\n",
       "        (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (proj): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (dec): Generator(\n",
       "    (conv_pre): Conv1d(256, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (ups): ModuleList(\n",
       "      (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(10,), padding=(3,))\n",
       "      (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "      (2): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(2,), padding=(3,))\n",
       "      (3): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))\n",
       "      (4): ConvTranspose1d(32, 16, kernel_size=(2,), stride=(2,))\n",
       "    )\n",
       "    (resblocks): ModuleList(\n",
       "      (0): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (6): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (7): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (8): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (9): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (10): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (11): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (12): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (13): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (14): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_post): Conv1d(16, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "    (cond): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (enc_q): PosteriorEncoder(\n",
       "    (pre): Conv1d(1025, 256, kernel_size=(1,), stride=(1,))\n",
       "    (enc): WN(\n",
       "      (in_layers): ModuleList(\n",
       "        (0-15): 16 x Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (res_skip_layers): ModuleList(\n",
       "        (0-14): 15 x Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "        (15): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "      (cond_layer): Conv1d(256, 8192, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (proj): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (flow): ResidualCouplingTransformersBlock(\n",
       "    (flows): ModuleList(\n",
       "      (0): FFTransformerCouplingLayer(\n",
       "        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "        (enc): FFT(\n",
       "          (cond_pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (cond_layer): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (self_attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_0): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(256, 768, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(768, 256, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (post): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): Flip()\n",
       "      (2): FFTransformerCouplingLayer(\n",
       "        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "        (enc): FFT(\n",
       "          (cond_pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (cond_layer): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (self_attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_0): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(256, 768, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(768, 256, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (post): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): Flip()\n",
       "      (4): FFTransformerCouplingLayer(\n",
       "        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "        (enc): FFT(\n",
       "          (cond_pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (cond_layer): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (self_attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_0): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(256, 768, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(768, 256, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (post): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (5): Flip()\n",
       "      (6): FFTransformerCouplingLayer(\n",
       "        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "        (enc): FFT(\n",
       "          (cond_pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (cond_layer): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (self_attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_0): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(256, 768, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(768, 256, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (post): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (7): Flip()\n",
       "    )\n",
       "  )\n",
       "  (ref_enc): MelStyleEncoder(\n",
       "    (spectral): Sequential(\n",
       "      (0): LinearNorm(\n",
       "        (fc): Linear(in_features=1025, out_features=128, bias=True)\n",
       "      )\n",
       "      (1): Mish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): LinearNorm(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (4): Mish()\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (temporal): Sequential(\n",
       "      (0): Conv1dGLU(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): Conv1dGLU(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (slf_attn): MultiHeadAttention(\n",
       "      (w_qs): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (w_ks): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (w_vs): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (softmax): Softmax(dim=2)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fc): LinearNorm(\n",
       "      (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "checkpoint = torch.load('./G_139548.pth')\n",
    "\n",
    "# 只获取模型状态字典\n",
    "model_state_dict = checkpoint['model']\n",
    "\n",
    "# 将状态字典加载到模型中\n",
    "net_g.load_state_dict(model_state_dict)\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "net_g.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "def load_audio(audio, target_sr):\n",
    "    speech, sample_rate = torchaudio.load(audio)\n",
    "    speech = speech.mean(dim=0, keepdim=True)\n",
    "    if sample_rate != target_sr:\n",
    "        speech = torchaudio.transforms.Resample(\n",
    "            orig_freq=sample_rate, new_freq=target_sr\n",
    "        )(speech)\n",
    "    return speech\n",
    "\n",
    "\n",
    "def postprocess(speech, target_sr, top_db=60, hop_length=220, win_length=440):\n",
    "    speech, _ = librosa.effects.trim(\n",
    "        speech, top_db=top_db, frame_length=win_length, hop_length=hop_length\n",
    "    )\n",
    "    max_val = 0.8\n",
    "    if speech.abs().max() > max_val:\n",
    "        speech = speech / speech.abs().max() * max_val\n",
    "    speech = torch.concat([speech, torch.zeros(1, int(target_sr * 0.2))], dim=1)\n",
    "    return speech\n",
    "target_sr = 16000\n",
    "audio = load_audio(\"1.mp3\", target_sr)\n",
    "audio = postprocess(audio, target_sr)\n",
    "audio_np = audio.squeeze().numpy()\n",
    "new_wav_path = \"1.mp3\" + \".wav\"\n",
    "wavfile.write(new_wav_path, target_sr, (audio_np * 32768).astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "target_sr = 32000\n",
    "n_fft = 2048\n",
    "hop_size = 640\n",
    "win_size = 640\n",
    "hann_window = {}\n",
    "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n",
    "    if torch.min(y) < -1.0:\n",
    "        print(\"min value is \", torch.min(y))\n",
    "    if torch.max(y) > 1.0:\n",
    "        print(\"max value is \", torch.max(y))\n",
    "\n",
    "    global hann_window\n",
    "    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n",
    "    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(\n",
    "            dtype=y.dtype, device=y.device\n",
    "        )\n",
    "\n",
    "    y = torch.nn.functional.pad(\n",
    "        y.unsqueeze(1),\n",
    "        (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "    y = y.squeeze(1)\n",
    "    spec = torch.stft(\n",
    "        y,\n",
    "        n_fft,\n",
    "        hop_length=hop_size,\n",
    "        win_length=win_size,\n",
    "        window=hann_window[wnsize_dtype_device],\n",
    "        center=center,\n",
    "        pad_mode=\"reflect\",\n",
    "        normalized=False,\n",
    "        onesided=True,\n",
    "        return_complex=False,\n",
    "    )\n",
    "\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "    return spec\n",
    "\n",
    "def load_audio(file, sr):\n",
    "    try:\n",
    "        # https://github.com/openai/whisper/blob/main/whisper/audio.py#L26\n",
    "        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n",
    "        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n",
    "        file = (\n",
    "            file.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n",
    "        )  # 防止小白拷路径头尾带了空格和\"和回车\n",
    "        out, _ = (\n",
    "            ffmpeg.input(file, threads=0)\n",
    "            .output(\"-\", format=\"f32le\", acodec=\"pcm_f32le\", ac=1, ar=sr)\n",
    "            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load audio: {e}\")\n",
    "\n",
    "    return np.frombuffer(out, np.float32).flatten()\n",
    "def get_audio(filename):\n",
    "    audio_array = load_audio(filename, target_sr)\n",
    "    audio = torch.FloatTensor(audio_array)\n",
    "    audio = audio.unsqueeze(0)\n",
    "    spec = spectrogram_torch(\n",
    "        audio,\n",
    "        n_fft,\n",
    "        target_sr,\n",
    "        hop_size,\n",
    "        win_size,\n",
    "        center=False,\n",
    "    )\n",
    "    spec = torch.squeeze(spec, 0)\n",
    "    return spec, audio\n",
    "\n",
    "y, audio = get_audio(\"./1.mp3.wav\")\n",
    "\n",
    "y_lengths = torch.tensor([y.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "text = \"用这令旗，不仅调兵遣将能胜人一筹，投降也能先人一步……\"\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"../pretrained_models/sentencepiece.bpe.model\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "        token = [0] + [x + 1 for x in sp.encode(text)] + [2]\n",
    "        return token\n",
    "    \n",
    "text_token = torch.Tensor(tokenize_text(text)).long()\n",
    "text_token_lengths = torch.tensor([len(text_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设置的min_length为: 341\n",
      "截断后speech_token的形状: torch.Size([341])\n",
      "截断后y的形状: torch.Size([1025, 341])\n"
     ]
    }
   ],
   "source": [
    "min_length = min(speech_token.shape[-1], y.shape[-1])\n",
    "\n",
    "print(f\"设置的min_length为: {min_length}\")\n",
    "\n",
    "# 使用最小值截断speech_token和y\n",
    "speech_token = speech_token[..., :min_length]\n",
    "y = y[..., :min_length]\n",
    "\n",
    "print(f\"截断后speech_token的形状: {speech_token.shape}\")\n",
    "print(f\"截断后y的形状: {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      6,   1173,   1842,  12668,  37175,      4,  21523,  17619,\n",
       "          19752, 153234,   1726,   1580,  27418,    487,    684,  94639,      4,\n",
       "          11537,  16328,  37894,   2996,    487,  55007,   2551,      2]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unsqueeze_(0)\n",
    "speech_token.unsqueeze_(0)\n",
    "text_token.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, _, _ = net_g.infer(\n",
    "    speech_token,\n",
    "    y,\n",
    "    y_lengths,\n",
    "    torch.zeros_like(text_token),\n",
    "    text_token_lengths,\n",
    "    noise_scale=0\n",
    ")\n",
    "\n",
    "# 确保音频数据是二维的 (channels, samples)\n",
    "if audio.dim() == 3:\n",
    "    audio = audio.squeeze(0)  # 移除批次维度\n",
    "\n",
    "# 设置采样率\n",
    "sample_rate = 32000\n",
    "\n",
    "# 生成文件名\n",
    "output_filename = f\"generated_audio1.wav\"\n",
    "\n",
    "# 保存音频文件\n",
    "torchaudio.save(output_filename, audio.cpu(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import traceback\n",
    "import numpy as np\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import librosa\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "from GPT_SoVITS.module.mel_processing import spectrogram_torch\n",
    "\n",
    "from tools.my_utils import load_audio\n",
    "\n",
    "class TextAudioSpeakerLoader(torch.utils.data.Dataset):\n",
    "    def tokenize_text(self, text):\n",
    "        token = [0] + [x + 1 for x in self.sp.encode(text)] + [2]\n",
    "        return token\n",
    "\n",
    "    def __init__(self, hparams, val=False):\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(\"../pretrained_models/sentencepiece.bpe.model\")\n",
    "        exp_dir = hparams.exp_dir\n",
    "        todo = []\n",
    "        self.audiopaths_text = []\n",
    "        self.lengths = []\n",
    "        for root, dirs, files in os.walk(exp_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    index_folder = os.path.relpath(root, exp_dir)\n",
    "                    file_path = os.path.join(root, file)\n",
    "\n",
    "                    # 尝试不同的编码\n",
    "                    encodings = [\"utf-8\", \"gbk\", \"gb2312\", \"utf-16\"]\n",
    "                    for encoding in encodings:\n",
    "                        try:\n",
    "                            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                                lines = f.readlines()\n",
    "                            break  # 如果成功读取，跳出循环\n",
    "                        except UnicodeDecodeError:\n",
    "                            continue  # 如果解码失败，尝试下一个编码\n",
    "                    else:\n",
    "                        print(f\"无法解码文件 {file_path}，跳过此文件\")\n",
    "                        continue  # 如果所有编码都失败，跳过此文件\n",
    "\n",
    "                    for line in lines:\n",
    "                        try:\n",
    "                            spk_name, wav_name, text = line.split(\"|\")\n",
    "                            todo.append([spk_name, wav_name, text, index_folder])\n",
    "                        except Exception:\n",
    "                            print(line)\n",
    "        for data in todo:\n",
    "            _, wav_name, text, index_folder = data\n",
    "            audio_path = os.path.join(exp_dir, index_folder, wav_name)\n",
    "            speech_token_path = audio_path + \".npy\"\n",
    "            bert_path = audio_path + \".pt\"\n",
    "            wav_path = audio_path + \".wav\"\n",
    "            if (\n",
    "                os.path.exists(speech_token_path)\n",
    "                and os.path.exists(bert_path)\n",
    "                and os.path.exists(wav_path)\n",
    "            ):\n",
    "                try:\n",
    "                    duration = librosa.get_duration(filename=wav_path)  # noqa: F821\n",
    "                    self.lengths.append(math.ceil(duration * 50))\n",
    "                except Exception as e:\n",
    "                    print(f\"无法处理文件 {wav_path}：{str(e)}\")\n",
    "                    continue\n",
    "                self.audiopaths_text.append([audio_path, text])\n",
    "\n",
    "        self.max_wav_value = hparams.max_wav_value\n",
    "        self.sampling_rate = hparams.sampling_rate\n",
    "        self.filter_length = hparams.filter_length\n",
    "        self.hop_length = hparams.hop_length\n",
    "        self.win_length = hparams.win_length\n",
    "        self.sampling_rate = hparams.sampling_rate\n",
    "        self.val = val\n",
    "\n",
    "        \"\"\"\n",
    "        @misc{picard2023torchmanualseed3407needinfluencerandom,\n",
    "        title={Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision}, \n",
    "        author={David Picard},\n",
    "        year={2023},\n",
    "        eprint={2109.08203},\n",
    "        archivePrefix={arXiv},\n",
    "        primaryClass={cs.CV},\n",
    "        url={https://arxiv.org/abs/2109.08203}, \n",
    "        }\n",
    "        \"\"\"\n",
    "        random.seed(3407)  # 3407 is all you need\n",
    "\n",
    "        random.shuffle(self.audiopaths_text)\n",
    "        print(\"wav_data_len:\", len(self.audiopaths_text))\n",
    "\n",
    "    def get_audio_text_speaker_pair(self, audiopath_text):\n",
    "        audiopath, text = audiopath_text\n",
    "        text_token = self.tokenize_text(text)\n",
    "        try:\n",
    "            spec, wav = self.get_audio(audiopath + \".wav\")\n",
    "            speech_token = np.load(audiopath + \".npy\")\n",
    "            speech_token = torch.from_numpy(speech_token)\n",
    "            min_length = min(speech_token.shape[-1], spec.shape[-1])\n",
    "            speech_token = speech_token[..., :min_length]\n",
    "            spec = spec[..., :min_length]\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            spec = torch.zeros(1025, 100)\n",
    "            wav = torch.zeros(1, 100 * self.hop_length)\n",
    "            speech_token = torch.zeros(1, 100)\n",
    "            text_token = text_token[-1:]\n",
    "            print(\"load error!!!!!!\", audiopath)\n",
    "        return (speech_token, spec, wav, text_token)\n",
    "\n",
    "    def get_audio(self, filename):\n",
    "        audio_array = load_audio(filename, self.sampling_rate)\n",
    "        audio = torch.FloatTensor(audio_array)\n",
    "        audio = audio.unsqueeze(0)\n",
    "        spec = spectrogram_torch(\n",
    "            audio,\n",
    "            self.filter_length,\n",
    "            self.sampling_rate,\n",
    "            self.hop_length,\n",
    "            self.win_length,\n",
    "            center=False,\n",
    "        )\n",
    "        spec = torch.squeeze(spec, 0)\n",
    "        return spec, audio\n",
    "\n",
    "    def get_sid(self, sid):\n",
    "        sid = torch.LongTensor([int(sid)])\n",
    "        return sid\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # with torch.no_grad():\n",
    "        return self.get_audio_text_speaker_pair(self.audiopaths_text[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audiopaths_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav_data_len: 16006\n",
      "数据集大小: 16006\n",
      "语音标记形状: torch.Size([324])\n",
      "频谱图形状: torch.Size([1025, 324])\n",
      "波形形状: torch.Size([1, 207920])\n",
      "文本标记: [0, 6, 10330, 216765, 886, 130668, 4, 35922, 274, 8856, 887, 7402, 83855, 4011, 25840, 4, 63211, 184120, 274, 2058, 4011, 2391, 4695, 6042, 4, 1036, 4502, 32, 2]\n",
      "音频已导出为 output_audio.wav\n"
     ]
    }
   ],
   "source": [
    "class SimpleHparams:\n",
    "    def __init__(self):\n",
    "        self.exp_dir = \"../dataset\"\n",
    "        self.max_wav_value = 32768.0\n",
    "        self.sampling_rate = 32000\n",
    "        self.filter_length = 2048\n",
    "        self.hop_length = 640\n",
    "        self.win_length = 2048\n",
    "\n",
    "# 创建 hparams 实例\n",
    "hparams = SimpleHparams()\n",
    "\n",
    "# 创建 TextAudioSpeakerLoader 实例\n",
    "dataset = TextAudioSpeakerLoader(hparams)\n",
    "\n",
    "# 打印数据集的大小\n",
    "print(f\"数据集大小: {len(dataset)}\")\n",
    "\n",
    "# 获取第一个样本\n",
    "if len(dataset) > 0:\n",
    "    sample = dataset[1]\n",
    "    speech_token, spec, wav, text_token = sample\n",
    "    \n",
    "    print(f\"语音标记形状: {speech_token.shape}\")\n",
    "    print(f\"频谱图形状: {spec.shape}\")\n",
    "    print(f\"波形形状: {wav.shape}\")\n",
    "    print(f\"文本标记: {text_token}\")\n",
    "    \n",
    "    # 保存wav到文件\n",
    "    import soundfile as sf\n",
    "    import numpy as np\n",
    "    \n",
    "    # 确保wav是一个numpy数组，并且是float32类型\n",
    "    if isinstance(wav, torch.Tensor):\n",
    "        wav = wav.numpy()\n",
    "    wav = wav.astype(np.float32)\n",
    "    \n",
    "    # 确保音频数据在-1到1之间\n",
    "    wav = np.clip(wav, -1, 1)\n",
    "    \n",
    "    # 导出为WAV文件\n",
    "    try:\n",
    "        sf.write('output_audio.wav', wav.squeeze(), hparams.sampling_rate, subtype='FLOAT')\n",
    "        print(\"音频已导出为 output_audio.wav\")\n",
    "    except Exception as e:\n",
    "        print(f\"导出音频时出错: {str(e)}\")\n",
    "else:\n",
    "    print(\"数据集为空\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([324])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAudioCollate:\n",
    "    def __init__(self, return_ids=False):\n",
    "        self.return_ids = return_ids\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # 按照频谱图长度排序\n",
    "        _, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([x[1].size(1) for x in batch]),\n",
    "            dim=0, descending=True\n",
    "        )\n",
    "\n",
    "        max_speech_len = max([x[0].size(0) for x in batch])\n",
    "        max_spec_len = max([x[1].size(1) for x in batch])\n",
    "        max_wav_len = max([x[2].size(1) for x in batch])\n",
    "        max_text_len = max([len(x[3]) for x in batch])\n",
    "\n",
    "        speech_lengths = torch.LongTensor(len(batch))\n",
    "        spec_lengths = torch.LongTensor(len(batch))\n",
    "        wav_lengths = torch.LongTensor(len(batch))\n",
    "        text_lengths = torch.LongTensor(len(batch))\n",
    "\n",
    "        speech_padded = torch.LongTensor(len(batch), max_speech_len)\n",
    "        spec_padded = torch.FloatTensor(len(batch), 1025, max_spec_len)\n",
    "        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n",
    "        text_padded = torch.LongTensor(len(batch), max_text_len)\n",
    "\n",
    "        speech_padded.zero_()\n",
    "        spec_padded.zero_()\n",
    "        wav_padded.zero_()\n",
    "        text_padded.zero_()\n",
    "\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            row = batch[ids_sorted_decreasing[i]]\n",
    "\n",
    "            speech = row[0]\n",
    "            speech_padded[i, :speech.size(0)] = speech\n",
    "            speech_lengths[i] = speech.size(0)\n",
    "\n",
    "            spec = row[1]\n",
    "            spec_padded[i, :, :spec.size(1)] = spec\n",
    "            spec_lengths[i] = spec.size(1)\n",
    "\n",
    "            wav = row[2]\n",
    "            wav_padded[i, :, :wav.size(1)] = wav\n",
    "            wav_lengths[i] = wav.size(1)\n",
    "\n",
    "            text = torch.LongTensor(row[3])\n",
    "            text_padded[i, :text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "\n",
    "        if self.return_ids:\n",
    "            return (\n",
    "                speech_padded, speech_lengths, spec_padded, spec_lengths,\n",
    "                wav_padded, wav_lengths, text_padded, text_lengths, ids_sorted_decreasing\n",
    "            )\n",
    "        return (\n",
    "            speech_padded, speech_lengths, spec_padded, spec_lengths,\n",
    "            wav_padded, wav_lengths, text_padded, text_lengths\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech_padded shape: torch.Size([4, 265]), type: torch.int64\n",
      "speech_lengths shape: torch.Size([4]), type: torch.int64\n",
      "spec_padded shape: torch.Size([4, 1025, 265]), type: torch.float32\n",
      "spec_lengths shape: torch.Size([4]), type: torch.int64\n",
      "wav_padded shape: torch.Size([4, 1, 170080]), type: torch.float32\n",
      "wav_lengths shape: torch.Size([4]), type: torch.int64\n",
      "text_padded shape: torch.Size([4, 20]), type: torch.int64\n",
      "text_lengths shape: torch.Size([4]), type: torch.int64\n",
      "\n",
      "Sample 0:\n",
      "  Speech length: 265, Padded speech shape: torch.Size([265])\n",
      "  Spec length: 265, Padded spec shape: torch.Size([1025, 265])\n",
      "  Wav length: 170080, Padded wav shape: torch.Size([1, 170080])\n",
      "  Text length: 20, Padded text shape: torch.Size([20])\n",
      "\n",
      "Sample 1:\n",
      "  Speech length: 199, Padded speech shape: torch.Size([265])\n",
      "  Spec length: 199, Padded spec shape: torch.Size([1025, 265])\n",
      "  Wav length: 130700, Padded wav shape: torch.Size([1, 170080])\n",
      "  Text length: 16, Padded text shape: torch.Size([20])\n",
      "\n",
      "Sample 2:\n",
      "  Speech length: 77, Padded speech shape: torch.Size([265])\n",
      "  Spec length: 77, Padded spec shape: torch.Size([1025, 265])\n",
      "  Wav length: 49443, Padded wav shape: torch.Size([1, 170080])\n",
      "  Text length: 8, Padded text shape: torch.Size([20])\n",
      "\n",
      "Sample 3:\n",
      "  Speech length: 60, Padded speech shape: torch.Size([265])\n",
      "  Spec length: 60, Padded spec shape: torch.Size([1025, 265])\n",
      "  Wav length: 38960, Padded wav shape: torch.Size([1, 170080])\n",
      "  Text length: 7, Padded text shape: torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# 创建 collate 函数实例\n",
    "collate_fn = TextAudioCollate()\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 4  # 使用小批量以便于观察\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# 获取一个批次的数据\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "# 解包批次数据\n",
    "speech_padded, speech_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, text_padded, text_lengths = batch\n",
    "\n",
    "# 打印每个张量的形状和类型\n",
    "print(f\"speech_padded shape: {speech_padded.shape}, type: {speech_padded.dtype}\")\n",
    "print(f\"speech_lengths shape: {speech_lengths.shape}, type: {speech_lengths.dtype}\")\n",
    "print(f\"spec_padded shape: {spec_padded.shape}, type: {spec_padded.dtype}\")\n",
    "print(f\"spec_lengths shape: {spec_lengths.shape}, type: {spec_lengths.dtype}\")\n",
    "print(f\"wav_padded shape: {wav_padded.shape}, type: {wav_padded.dtype}\")\n",
    "print(f\"wav_lengths shape: {wav_lengths.shape}, type: {wav_lengths.dtype}\")\n",
    "print(f\"text_padded shape: {text_padded.shape}, type: {text_padded.dtype}\")\n",
    "print(f\"text_lengths shape: {text_lengths.shape}, type: {text_lengths.dtype}\")\n",
    "\n",
    "# 验证填充是否正确\n",
    "for i in range(batch_size):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  Speech length: {speech_lengths[i]}, Padded speech shape: {speech_padded[i].shape}\")\n",
    "    print(f\"  Spec length: {spec_lengths[i]}, Padded spec shape: {spec_padded[i].shape}\")\n",
    "    print(f\"  Wav length: {wav_lengths[i]}, Padded wav shape: {wav_padded[i].shape}\")\n",
    "    print(f\"  Text length: {text_lengths[i]}, Padded text shape: {text_padded[i].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "音频 0 已保存为 generated_audio_0.wav\n",
      "音频 1 已保存为 generated_audio_1.wav\n",
      "音频 2 已保存为 generated_audio_2.wav\n",
      "音频 3 已保存为 generated_audio_3.wav\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    audio, _, _ = net_g.infer(\n",
    "        speech_padded[i:i+1],\n",
    "        spec_padded[i:i+1],\n",
    "        spec_lengths[i:i+1],\n",
    "        text_padded[i:i+1],\n",
    "        text_lengths[i:i+1],\n",
    "    )\n",
    "    \n",
    "    # 确保音频数据是二维的 (channels, samples)\n",
    "    if audio.dim() == 3:\n",
    "        audio = audio.squeeze(0)  # 移除批次维度\n",
    "\n",
    "    # 设置采样率\n",
    "    sample_rate = 32000\n",
    "\n",
    "    # 生成文件名\n",
    "    output_filename = f\"generated_audio_{i}.wav\"\n",
    "\n",
    "    # 保存音频文件\n",
    "    torchaudio.save(output_filename, audio.cpu(), sample_rate)\n",
    "\n",
    "    print(f\"音频 {i} 已保存为 {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 272])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_padded[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1025, 272])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_padded[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_lengths[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_padded[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, _, _ = net_g.infer(\n",
    "    speech_padded[0:1],\n",
    "    spec_padded[3:4],\n",
    "    spec_lengths[0:1],\n",
    "    text_padded[2:3],\n",
    "    text_lengths[2:3],\n",
    ")\n",
    "\n",
    "# 确保音频数据是二维的 (channels, samples)\n",
    "if audio.dim() == 3:\n",
    "    audio = audio.squeeze(0)  # 移除批次维度\n",
    "\n",
    "# 设置采样率\n",
    "sample_rate = 32000\n",
    "\n",
    "# 生成文件名\n",
    "output_filename = f\"generated_audio.wav\"\n",
    "\n",
    "# 保存音频文件\n",
    "torchaudio.save(output_filename, audio.cpu(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 174080])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "音频已保存为 generated_audio.wav\n"
     ]
    }
   ],
   "source": [
    "# 保存生成的音频\n",
    "import torchaudio\n",
    "\n",
    "# 确保音频数据是二维的 (channels, samples)\n",
    "if audio.dim() == 3:\n",
    "    audio = audio.squeeze(0)  # 移除批次维度\n",
    "\n",
    "# 设置采样率（假设为22050Hz，如果不同请调整）\n",
    "sample_rate = 32000\n",
    "\n",
    "# 生成文件名\n",
    "output_filename = \"generated_audio.wav\"\n",
    "\n",
    "# 保存音频文件\n",
    "torchaudio.save(output_filename, audio.cpu(), sample_rate)\n",
    "\n",
    "print(f\"音频已保存为 {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 159740])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_padded[0:1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
