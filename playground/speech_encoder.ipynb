{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import librosa\n",
    "import torch\n",
    "max_val = 0.8\n",
    "target_sr = 16000\n",
    "def load_wav(wav, target_sr):\n",
    "    speech, sample_rate = torchaudio.load(wav)\n",
    "    speech = speech.mean(dim=0, keepdim=True)\n",
    "    if sample_rate != target_sr:\n",
    "        assert sample_rate > target_sr, 'wav sample rate {} must be greater than {}'.format(sample_rate, target_sr)\n",
    "        speech = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)(speech)\n",
    "    return speech\n",
    "def postprocess(speech, top_db=60, hop_length=220, win_length=440):\n",
    "    speech, _ = librosa.effects.trim(\n",
    "        speech, top_db=top_db,\n",
    "        frame_length=win_length,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    if speech.abs().max() > max_val:\n",
    "        speech = speech / speech.abs().max() * max_val\n",
    "    speech = torch.concat([speech, torch.zeros(1, int(target_sr * 0.2))], dim=1)\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16k音频已保存为 '花火_16k.wav'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "speech_16k = postprocess(load_wav(\"./花火.mp3\", 16000))\n",
    "#导出16k的音频\n",
    "# 导出16k的音频\n",
    "torchaudio.save(\"花火_16k.wav\", speech_16k, target_sr)\n",
    "print(\"16k音频已保存为 '花火_16k.wav'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语音时长: 7.4325 秒\n"
     ]
    }
   ],
   "source": [
    "# 计算语音时长\n",
    "audio_duration = speech_16k.shape[1] / target_sr\n",
    "\n",
    "print(f\"语音时长: {audio_duration:.4f} 秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "feat = torch.zeros(1, 207920//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "# load \n",
    "feat = whisper.log_mel_spectrogram(feat, n_mels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-24 15:39:00.404377785 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 12 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-07-24 15:39:00.405305975 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-07-24 15:39:00.405310589 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "option = onnxruntime.SessionOptions()\n",
    "option.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "option.intra_op_num_threads = 1\n",
    "speech_tokenizer_session = onnxruntime.InferenceSession(\n",
    "    \"../pretrained_models/speech_tokenizer_v1.onnx\", sess_options=option, providers=[\"CUDAExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats\n",
      "feats_length\n"
     ]
    }
   ],
   "source": [
    "print(speech_tokenizer_session.get_inputs()[0].name)\n",
    "print(speech_tokenizer_session.get_inputs()[1].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "        [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "        [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "        ...,\n",
       "        [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "        [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5],\n",
       "        [-1.5, -1.5, -1.5, ..., -1.5, -1.5, -1.5]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_length = np.array([feat.shape[2]], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_length: [649]\n"
     ]
    }
   ],
   "source": [
    "print(f\"feat_length: {feat_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_token = speech_tokenizer_session.run(None, {speech_tokenizer_session.get_inputs()[0].name: feat.detach().cpu().numpy(),\n",
    "                                                                speech_tokenizer_session.get_inputs()[1].name: np.array([feat.shape[2]], dtype=np.int32)})[0].flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = np.array(speech_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = torch.zeros(1, 207920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1025, 325])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=True):\n",
    "    # 确保输入信号在合理范围内\n",
    "    if torch.min(y) < -1.0 or torch.max(y) > 1.0:\n",
    "        print(f\"警告: 输入信号超出[-1, 1]范围. 最小值: {torch.min(y)}, 最大值: {torch.max(y)}\")\n",
    "    \n",
    "    # 创建汉宁窗\n",
    "    window = torch.hann_window(win_size, dtype=y.dtype, device=y.device)\n",
    "    \n",
    "    # 对信号进行填充\n",
    "    pad_len = (n_fft - hop_size) // 2\n",
    "    y_padded = F.pad(y.unsqueeze(1), (pad_len, pad_len), mode='reflect').squeeze(1)\n",
    "    \n",
    "    # 执行短时傅里叶变换 (STFT)\n",
    "    stft_matrix = torch.stft(\n",
    "        y_padded,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_size,\n",
    "        win_length=win_size,\n",
    "        window=window,\n",
    "        center=center,\n",
    "        normalized=False,\n",
    "        onesided=True,\n",
    "        return_complex=True\n",
    "    )\n",
    "    \n",
    "    # 计算幅度谱\n",
    "    magnitudes = torch.abs(stft_matrix)\n",
    "    \n",
    "    # 添加小量值以避免取对数时出现问题\n",
    "    spec = torch.log1p(magnitudes)\n",
    "    \n",
    "    return spec\n",
    "spec = spectrogram_torch(\n",
    "            feat,\n",
    "            2048,\n",
    "            32000,\n",
    "            640,\n",
    "            2048,\n",
    "            center=False,\n",
    "        )\n",
    "spec = torch.squeeze(spec, 0)\n",
    "print(spec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rotary_embedding_torch import RotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotary_emb = RotaryEmbedding(dim = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 99999, 768])\n"
     ]
    }
   ],
   "source": [
    "q = torch.ones(1, 99999, 768)\n",
    "q = rotary_emb.rotate_queries_or_keys(q)\n",
    "print(q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3012,  1.3818, -0.2683,  1.3885, -0.2361,  1.3944, -0.2046,  1.3993,\n",
       "        -0.1737,  1.4035, -0.1434,  1.4069, -0.1138,  1.4096, -0.0849,  1.4117,\n",
       "        -0.0566,  1.4131, -0.0289,  1.4139, -0.0019,  1.4142,  0.0245,  1.4140,\n",
       "         0.0502,  1.4133,  0.0753,  1.4122,  0.0998,  1.4107,  0.1237,  1.4088,\n",
       "         0.1470,  1.4066,  0.1697,  1.4040,  0.1918,  1.4012,  0.2133,  1.3980,\n",
       "         0.2343,  1.3947,  0.2547,  1.3911,  0.2746,  1.3873,  0.2940,  1.3833,\n",
       "         0.3128,  1.3792,  0.3312,  1.3749,  0.3491,  1.3705,  0.3664,  1.3659,\n",
       "         0.3833,  1.3613,  0.3998,  1.3565,  0.4158,  1.3517,  0.4314,  1.3468,\n",
       "         0.4465,  1.3419,  0.4613,  1.3369,  0.4756,  1.3318,  0.4895,  1.3268,\n",
       "         0.5031,  1.3217,  0.5163,  1.3166,  0.5291,  1.3115,  0.5416,  1.3064,\n",
       "         0.5537,  1.3013,  0.5655,  1.2962,  0.5769,  1.2912,  0.5881,  1.2861,\n",
       "         0.5989,  1.2811,  0.6095,  1.2761,  0.6198,  1.2712,  0.6297,  1.2663,\n",
       "         0.6394,  1.2614,  0.6489,  1.2566,  0.6580,  1.2518,  0.6670,  1.2471,\n",
       "         0.6757,  1.2424,  0.6841,  1.2377,  0.6923,  1.2332,  0.7003,  1.2287,\n",
       "         0.7081,  1.2242,  0.7156,  1.2198,  0.7230,  1.2154,  0.7301,  1.2112,\n",
       "         0.7371,  1.2069,  0.7439,  1.2028,  0.7505,  1.1987,  0.7569,  1.1946,\n",
       "         0.7631,  1.1907,  0.7692,  1.1868,  0.7751,  1.1829,  0.7808,  1.1791,\n",
       "         0.7864,  1.1754,  0.7919,  1.1717,  0.7972,  1.1681,  0.8023,  1.1646,\n",
       "         0.8073,  1.1611,  0.8122,  1.1577,  0.8170,  1.1544,  0.8216,  1.1511,\n",
       "         0.8261,  1.1478,  0.8305,  1.1447,  0.8348,  1.1416,  0.8389,  1.1385,\n",
       "         0.8430,  1.1355,  0.8469,  1.1326,  0.8508,  1.1297,  0.8545,  1.1269,\n",
       "         0.8582,  1.1241,  0.8617,  1.1214,  0.8652,  1.1187,  0.8685,  1.1161,\n",
       "         0.8718,  1.1135,  0.8750,  1.1110,  0.8781,  1.1086,  0.8811,  1.1062,\n",
       "         0.8841,  1.1038,  0.8870,  1.1015,  0.8898,  1.0992,  0.8925,  1.0970,\n",
       "         0.8952,  1.0948,  0.8978,  1.0927,  0.9003,  1.0906,  0.9028,  1.0886,\n",
       "         0.9051,  1.0866,  0.9075,  1.0847,  0.9098,  1.0827,  0.9120,  1.0809,\n",
       "         0.9141,  1.0790,  0.9163,  1.0773,  0.9183,  1.0755,  0.9203,  1.0738,\n",
       "         0.9223,  1.0721,  0.9242,  1.0705,  0.9260,  1.0689,  0.9278,  1.0673,\n",
       "         0.9296,  1.0658,  0.9313,  1.0643,  0.9330,  1.0628,  0.9346,  1.0613,\n",
       "         0.9362,  1.0599,  0.9378,  1.0586,  0.9393,  1.0572,  0.9408,  1.0559,\n",
       "         0.9422,  1.0546,  0.9436,  1.0534,  0.9450,  1.0521,  0.9463,  1.0509,\n",
       "         0.9476,  1.0498,  0.9489,  1.0486,  0.9501,  1.0475,  0.9513,  1.0464,\n",
       "         0.9525,  1.0453,  0.9537,  1.0443,  0.9548,  1.0432,  0.9559,  1.0422,\n",
       "         0.9570,  1.0413,  0.9580,  1.0403,  0.9590,  1.0394,  0.9600,  1.0385,\n",
       "         0.9610,  1.0376,  0.9619,  1.0367,  0.9628,  1.0358,  0.9637,  1.0350,\n",
       "         0.9646,  1.0342,  0.9654,  1.0334,  0.9663,  1.0326,  0.9671,  1.0319,\n",
       "         0.9679,  1.0311,  0.9687,  1.0304,  0.9694,  1.0297,  0.9701,  1.0290,\n",
       "         0.9709,  1.0283,  0.9716,  1.0277,  0.9722,  1.0270,  0.9729,  1.0264,\n",
       "         0.9736,  1.0258,  0.9742,  1.0252,  0.9748,  1.0246,  0.9754,  1.0240,\n",
       "         0.9760,  1.0234,  0.9766,  1.0229,  0.9771,  1.0223,  0.9777,  1.0218,\n",
       "         0.9782,  1.0213,  0.9787,  1.0208,  0.9793,  1.0203,  0.9798,  1.0198,\n",
       "         0.9802,  1.0194,  0.9807,  1.0189,  0.9812,  1.0185,  0.9816,  1.0180,\n",
       "         0.9821,  1.0176,  0.9825,  1.0172,  0.9829,  1.0168,  0.9833,  1.0164,\n",
       "         0.9837,  1.0160,  0.9841,  1.0156,  0.9845,  1.0153,  0.9849,  1.0149,\n",
       "         0.9852,  1.0146,  0.9856,  1.0142,  0.9859,  1.0139,  0.9862,  1.0136,\n",
       "         0.9866,  1.0132,  0.9869,  1.0129,  0.9872,  1.0126,  0.9875,  1.0123,\n",
       "         0.9878,  1.0120,  0.9881,  1.0118,  0.9884,  1.0115,  0.9887,  1.0112,\n",
       "         0.9889,  1.0109,  0.9892,  1.0107,  0.9895,  1.0104,  0.9897,  1.0102,\n",
       "         0.9900,  1.0099,  0.9902,  1.0097,  0.9904,  1.0095,  0.9907,  1.0093,\n",
       "         0.9909,  1.0090,  0.9911,  1.0088,  0.9913,  1.0086,  0.9915,  1.0084,\n",
       "         0.9917,  1.0082,  0.9919,  1.0080,  0.9921,  1.0078,  0.9923,  1.0077,\n",
       "         0.9925,  1.0075,  0.9927,  1.0073,  0.9928,  1.0071,  0.9930,  1.0070,\n",
       "         0.9932,  1.0068,  0.9933,  1.0066,  0.9935,  1.0065,  0.9936,  1.0063,\n",
       "         0.9938,  1.0062,  0.9939,  1.0060,  0.9941,  1.0059,  0.9942,  1.0057,\n",
       "         0.9944,  1.0056,  0.9945,  1.0055,  0.9946,  1.0053,  0.9948,  1.0052,\n",
       "         0.9949,  1.0051,  0.9950,  1.0050,  0.9951,  1.0049,  0.9952,  1.0047,\n",
       "         0.9953,  1.0046,  0.9955,  1.0045,  0.9956,  1.0044,  0.9957,  1.0043,\n",
       "         0.9958,  1.0042,  0.9959,  1.0041,  0.9960,  1.0040,  0.9961,  1.0039,\n",
       "         0.9962,  1.0038,  0.9963,  1.0037,  0.9963,  1.0036,  0.9964,  1.0036,\n",
       "         0.9965,  1.0035,  0.9966,  1.0034,  0.9967,  1.0033,  0.9968,  1.0032,\n",
       "         0.9968,  1.0032,  0.9969,  1.0031,  0.9970,  1.0030,  0.9971,  1.0029,\n",
       "         0.9971,  1.0029,  0.9972,  1.0028,  0.9973,  1.0027,  0.9973,  1.0027,\n",
       "         0.9974,  1.0026,  0.9974,  1.0025,  0.9975,  1.0025,  0.9976,  1.0024,\n",
       "         0.9976,  1.0024,  0.9977,  1.0023,  0.9977,  1.0023,  0.9978,  1.0022,\n",
       "         0.9978,  1.0022,  0.9979,  1.0021,  0.9979,  1.0021,  0.9980,  1.0020,\n",
       "         0.9980,  1.0020,  0.9981,  1.0019,  0.9981,  1.0019,  0.9982,  1.0018,\n",
       "         0.9982,  1.0018,  0.9983,  1.0017,  0.9983,  1.0017,  0.9983,  1.0017,\n",
       "         0.9984,  1.0016,  0.9984,  1.0016,  0.9985,  1.0015,  0.9985,  1.0015,\n",
       "         0.9985,  1.0015,  0.9986,  1.0014,  0.9986,  1.0014,  0.9986,  1.0014,\n",
       "         0.9987,  1.0013,  0.9987,  1.0013,  0.9987,  1.0013,  0.9988,  1.0012,\n",
       "         0.9988,  1.0012,  0.9988,  1.0012,  0.9988,  1.0012,  0.9989,  1.0011,\n",
       "         0.9989,  1.0011,  0.9989,  1.0011,  0.9990,  1.0010,  0.9990,  1.0010,\n",
       "         0.9990,  1.0010,  0.9990,  1.0010,  0.9990,  1.0010,  0.9991,  1.0009,\n",
       "         0.9991,  1.0009,  0.9991,  1.0009,  0.9991,  1.0009,  0.9992,  1.0008,\n",
       "         0.9992,  1.0008,  0.9992,  1.0008,  0.9992,  1.0008,  0.9992,  1.0008,\n",
       "         0.9992,  1.0007,  0.9993,  1.0007,  0.9993,  1.0007,  0.9993,  1.0007,\n",
       "         0.9993,  1.0007,  0.9993,  1.0007,  0.9994,  1.0006,  0.9994,  1.0006,\n",
       "         0.9994,  1.0006,  0.9994,  1.0006,  0.9994,  1.0006,  0.9994,  1.0006,\n",
       "         0.9994,  1.0006,  0.9995,  1.0005,  0.9995,  1.0005,  0.9995,  1.0005,\n",
       "         0.9995,  1.0005,  0.9995,  1.0005,  0.9995,  1.0005,  0.9995,  1.0005,\n",
       "         0.9995,  1.0005,  0.9995,  1.0005,  0.9996,  1.0004,  0.9996,  1.0004,\n",
       "         0.9996,  1.0004,  0.9996,  1.0004,  0.9996,  1.0004,  0.9996,  1.0004,\n",
       "         0.9996,  1.0004,  0.9996,  1.0004,  0.9996,  1.0004,  0.9996,  1.0004,\n",
       "         0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,\n",
       "         0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,\n",
       "         0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,\n",
       "         0.9997,  1.0003,  0.9997,  1.0003,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,\n",
       "         0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,\n",
       "         0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,\n",
       "         0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 99, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.3012,  1.3818, -0.2683,  1.3885, -0.2361,  1.3944, -0.2046,  1.3993,\n",
       "        -0.1737,  1.4035, -0.1434,  1.4069, -0.1138,  1.4096, -0.0849,  1.4117,\n",
       "        -0.0566,  1.4131, -0.0289,  1.4139, -0.0019,  1.4142,  0.0245,  1.4140,\n",
       "         0.0502,  1.4133,  0.0753,  1.4122,  0.0998,  1.4107,  0.1237,  1.4088,\n",
       "         0.1470,  1.4066,  0.1697,  1.4040,  0.1918,  1.4012,  0.2133,  1.3980,\n",
       "         0.2343,  1.3947,  0.2547,  1.3911,  0.2746,  1.3873,  0.2940,  1.3833,\n",
       "         0.3128,  1.3792,  0.3312,  1.3749,  0.3491,  1.3705,  0.3664,  1.3659,\n",
       "         0.3833,  1.3613,  0.3998,  1.3565,  0.4158,  1.3517,  0.4314,  1.3468,\n",
       "         0.4465,  1.3419,  0.4613,  1.3369,  0.4756,  1.3318,  0.4895,  1.3268,\n",
       "         0.5031,  1.3217,  0.5163,  1.3166,  0.5291,  1.3115,  0.5416,  1.3064,\n",
       "         0.5537,  1.3013,  0.5655,  1.2962,  0.5769,  1.2912,  0.5881,  1.2861,\n",
       "         0.5989,  1.2811,  0.6095,  1.2761,  0.6198,  1.2712,  0.6297,  1.2663,\n",
       "         0.6394,  1.2614,  0.6489,  1.2566,  0.6580,  1.2518,  0.6670,  1.2471,\n",
       "         0.6757,  1.2424,  0.6841,  1.2377,  0.6923,  1.2332,  0.7003,  1.2287,\n",
       "         0.7081,  1.2242,  0.7156,  1.2198,  0.7230,  1.2154,  0.7301,  1.2112,\n",
       "         0.7371,  1.2069,  0.7439,  1.2028,  0.7505,  1.1987,  0.7569,  1.1946,\n",
       "         0.7631,  1.1907,  0.7692,  1.1868,  0.7751,  1.1829,  0.7808,  1.1791,\n",
       "         0.7864,  1.1754,  0.7919,  1.1717,  0.7972,  1.1681,  0.8023,  1.1646,\n",
       "         0.8073,  1.1611,  0.8122,  1.1577,  0.8170,  1.1544,  0.8216,  1.1511,\n",
       "         0.8261,  1.1478,  0.8305,  1.1447,  0.8348,  1.1416,  0.8389,  1.1385,\n",
       "         0.8430,  1.1355,  0.8469,  1.1326,  0.8508,  1.1297,  0.8545,  1.1269,\n",
       "         0.8582,  1.1241,  0.8617,  1.1214,  0.8652,  1.1187,  0.8685,  1.1161,\n",
       "         0.8718,  1.1135,  0.8750,  1.1110,  0.8781,  1.1086,  0.8811,  1.1062,\n",
       "         0.8841,  1.1038,  0.8870,  1.1015,  0.8898,  1.0992,  0.8925,  1.0970,\n",
       "         0.8952,  1.0948,  0.8978,  1.0927,  0.9003,  1.0906,  0.9028,  1.0886,\n",
       "         0.9051,  1.0866,  0.9075,  1.0847,  0.9098,  1.0827,  0.9120,  1.0809,\n",
       "         0.9141,  1.0790,  0.9163,  1.0773,  0.9183,  1.0755,  0.9203,  1.0738,\n",
       "         0.9223,  1.0721,  0.9242,  1.0705,  0.9260,  1.0689,  0.9278,  1.0673,\n",
       "         0.9296,  1.0658,  0.9313,  1.0643,  0.9330,  1.0628,  0.9346,  1.0613,\n",
       "         0.9362,  1.0599,  0.9378,  1.0586,  0.9393,  1.0572,  0.9408,  1.0559,\n",
       "         0.9422,  1.0546,  0.9436,  1.0534,  0.9450,  1.0521,  0.9463,  1.0509,\n",
       "         0.9476,  1.0498,  0.9489,  1.0486,  0.9501,  1.0475,  0.9513,  1.0464,\n",
       "         0.9525,  1.0453,  0.9537,  1.0443,  0.9548,  1.0432,  0.9559,  1.0422,\n",
       "         0.9570,  1.0413,  0.9580,  1.0403,  0.9590,  1.0394,  0.9600,  1.0385,\n",
       "         0.9610,  1.0376,  0.9619,  1.0367,  0.9628,  1.0358,  0.9637,  1.0350,\n",
       "         0.9646,  1.0342,  0.9654,  1.0334,  0.9663,  1.0326,  0.9671,  1.0319,\n",
       "         0.9679,  1.0311,  0.9687,  1.0304,  0.9694,  1.0297,  0.9701,  1.0290,\n",
       "         0.9709,  1.0283,  0.9716,  1.0277,  0.9722,  1.0270,  0.9729,  1.0264,\n",
       "         0.9736,  1.0258,  0.9742,  1.0252,  0.9748,  1.0246,  0.9754,  1.0240,\n",
       "         0.9760,  1.0234,  0.9766,  1.0229,  0.9771,  1.0223,  0.9777,  1.0218,\n",
       "         0.9782,  1.0213,  0.9787,  1.0208,  0.9793,  1.0203,  0.9798,  1.0198,\n",
       "         0.9802,  1.0194,  0.9807,  1.0189,  0.9812,  1.0185,  0.9816,  1.0180,\n",
       "         0.9821,  1.0176,  0.9825,  1.0172,  0.9829,  1.0168,  0.9833,  1.0164,\n",
       "         0.9837,  1.0160,  0.9841,  1.0156,  0.9845,  1.0153,  0.9849,  1.0149,\n",
       "         0.9852,  1.0146,  0.9856,  1.0142,  0.9859,  1.0139,  0.9862,  1.0136,\n",
       "         0.9866,  1.0132,  0.9869,  1.0129,  0.9872,  1.0126,  0.9875,  1.0123,\n",
       "         0.9878,  1.0120,  0.9881,  1.0118,  0.9884,  1.0115,  0.9887,  1.0112,\n",
       "         0.9889,  1.0109,  0.9892,  1.0107,  0.9895,  1.0104,  0.9897,  1.0102,\n",
       "         0.9900,  1.0099,  0.9902,  1.0097,  0.9904,  1.0095,  0.9907,  1.0093,\n",
       "         0.9909,  1.0090,  0.9911,  1.0088,  0.9913,  1.0086,  0.9915,  1.0084,\n",
       "         0.9917,  1.0082,  0.9919,  1.0080,  0.9921,  1.0078,  0.9923,  1.0077,\n",
       "         0.9925,  1.0075,  0.9927,  1.0073,  0.9928,  1.0071,  0.9930,  1.0070,\n",
       "         0.9932,  1.0068,  0.9933,  1.0066,  0.9935,  1.0065,  0.9936,  1.0063,\n",
       "         0.9938,  1.0062,  0.9939,  1.0060,  0.9941,  1.0059,  0.9942,  1.0057,\n",
       "         0.9944,  1.0056,  0.9945,  1.0055,  0.9946,  1.0053,  0.9948,  1.0052,\n",
       "         0.9949,  1.0051,  0.9950,  1.0050,  0.9951,  1.0049,  0.9952,  1.0047,\n",
       "         0.9953,  1.0046,  0.9955,  1.0045,  0.9956,  1.0044,  0.9957,  1.0043,\n",
       "         0.9958,  1.0042,  0.9959,  1.0041,  0.9960,  1.0040,  0.9961,  1.0039,\n",
       "         0.9962,  1.0038,  0.9963,  1.0037,  0.9963,  1.0036,  0.9964,  1.0036,\n",
       "         0.9965,  1.0035,  0.9966,  1.0034,  0.9967,  1.0033,  0.9968,  1.0032,\n",
       "         0.9968,  1.0032,  0.9969,  1.0031,  0.9970,  1.0030,  0.9971,  1.0029,\n",
       "         0.9971,  1.0029,  0.9972,  1.0028,  0.9973,  1.0027,  0.9973,  1.0027,\n",
       "         0.9974,  1.0026,  0.9974,  1.0025,  0.9975,  1.0025,  0.9976,  1.0024,\n",
       "         0.9976,  1.0024,  0.9977,  1.0023,  0.9977,  1.0023,  0.9978,  1.0022,\n",
       "         0.9978,  1.0022,  0.9979,  1.0021,  0.9979,  1.0021,  0.9980,  1.0020,\n",
       "         0.9980,  1.0020,  0.9981,  1.0019,  0.9981,  1.0019,  0.9982,  1.0018,\n",
       "         0.9982,  1.0018,  0.9983,  1.0017,  0.9983,  1.0017,  0.9983,  1.0017,\n",
       "         0.9984,  1.0016,  0.9984,  1.0016,  0.9985,  1.0015,  0.9985,  1.0015,\n",
       "         0.9985,  1.0015,  0.9986,  1.0014,  0.9986,  1.0014,  0.9986,  1.0014,\n",
       "         0.9987,  1.0013,  0.9987,  1.0013,  0.9987,  1.0013,  0.9988,  1.0012,\n",
       "         0.9988,  1.0012,  0.9988,  1.0012,  0.9988,  1.0012,  0.9989,  1.0011,\n",
       "         0.9989,  1.0011,  0.9989,  1.0011,  0.9990,  1.0010,  0.9990,  1.0010,\n",
       "         0.9990,  1.0010,  0.9990,  1.0010,  0.9990,  1.0010,  0.9991,  1.0009,\n",
       "         0.9991,  1.0009,  0.9991,  1.0009,  0.9991,  1.0009,  0.9992,  1.0008,\n",
       "         0.9992,  1.0008,  0.9992,  1.0008,  0.9992,  1.0008,  0.9992,  1.0008,\n",
       "         0.9992,  1.0007,  0.9993,  1.0007,  0.9993,  1.0007,  0.9993,  1.0007,\n",
       "         0.9993,  1.0007,  0.9993,  1.0007,  0.9994,  1.0006,  0.9994,  1.0006,\n",
       "         0.9994,  1.0006,  0.9994,  1.0006,  0.9994,  1.0006,  0.9994,  1.0006,\n",
       "         0.9994,  1.0006,  0.9995,  1.0005,  0.9995,  1.0005,  0.9995,  1.0005,\n",
       "         0.9995,  1.0005,  0.9995,  1.0005,  0.9995,  1.0005,  0.9995,  1.0005,\n",
       "         0.9995,  1.0005,  0.9995,  1.0005,  0.9996,  1.0004,  0.9996,  1.0004,\n",
       "         0.9996,  1.0004,  0.9996,  1.0004,  0.9996,  1.0004,  0.9996,  1.0004,\n",
       "         0.9996,  1.0004,  0.9996,  1.0004,  0.9996,  1.0004,  0.9996,  1.0004,\n",
       "         0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,\n",
       "         0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,\n",
       "         0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,  0.9997,  1.0003,\n",
       "         0.9997,  1.0003,  0.9997,  1.0003,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,  0.9998,  1.0002,\n",
       "         0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,\n",
       "         0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,\n",
       "         0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,\n",
       "         0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001,  0.9999,  1.0001])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.ones(1, 99, 768)\n",
    "k = rotary_emb.rotate_queries_or_keys(k)\n",
    "print(k.shape)\n",
    "k[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
