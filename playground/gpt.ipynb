{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"./sentencepiece.bpe.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35377, 6660]\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"Hello World\")\n",
    "print(ids)\n",
    "\n",
    "# 解码(将ID转换回文本)\n",
    "text = sp.decode(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.nn.Embedding(250000, 256)\n",
    "emb.weight.data.uniform_(-0.1, 0.1)\n",
    "emb.weight.data.shape\n",
    "embedding = emb(torch.tensor(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前目录: /workspaces/GPT-SoVITS2/playground\n",
      "/workspaces/GPT-SoVITS2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(\"当前目录:\", current_dir)\n",
    "# 获取当前文件的父目录\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "print(parent_dir)\n",
    "from GPT_SoVITS.AR.models.t2s_model import Text2SemanticDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Text2SemanticDecoder({\n",
    "    \"model\": {\n",
    "        \"embedding_dim\": 768,\n",
    "        \"text_vocab_size\": 250000,\n",
    "        \"speech_vocab_size\": 4097,\n",
    "        \"bert_dim\": 1024\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, x_lens, y, y_lens, prompt_x, prompt_x_lens, prompt_y, prompt_y_lens, x_bert_feature, prompt_x_bert_feature\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "    [[2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 1, 1, 1, 1, 1, 4096, 4096, 4096, 4096]]\n",
    ")\n",
    "x_lens = torch.tensor([10, 6])\n",
    "y = torch.tensor(\n",
    "    [\n",
    "        [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4096, 4096],\n",
    "    ]\n",
    ")\n",
    "y_lens = torch.tensor([15, 13])\n",
    "prompt_x = torch.tensor(\n",
    "    [[3, 2, 2, 2, 2, 2, 2, 2, 2, 2], [3, 2, 2, 2, 2, 2, 4096, 4096, 4096, 4096]]\n",
    ")\n",
    "prompt_x_lens = torch.tensor([10, 6])\n",
    "prompt_y = torch.tensor(\n",
    "    [\n",
    "        [3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        [3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4096, 4096],\n",
    "    ]\n",
    ")\n",
    "prompt_y_lens = torch.tensor([15, 13])\n",
    "x_bert_feature = torch.zeros(2, 10, 1024)\n",
    "prompt_x_bert_feature = torch.zeros(2, 10, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6262, -1.1546, -1.1546, -1.1546, -1.1546, -1.1546, -1.1546, -1.1546,\n",
      "         -1.1546, -1.1546,  0.7911, -0.2122, -0.6793, -0.6793, -0.6793, -0.6793,\n",
      "         -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793,\n",
      "         -0.6793, -0.6793,  0.7911, -1.1546,  1.2042,  1.2042,  1.2042,  1.2042,\n",
      "          1.2042,  1.2042,  1.2042,  1.2042,  1.2042,  0.7911, -0.6793,  0.4717,\n",
      "          0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,\n",
      "          0.4717,  0.4717,  0.4717,  0.4717,  0.4717],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, -1.6262, -1.1546, -1.1546, -1.1546,\n",
      "         -1.1546, -1.1546,  0.7911, -0.2122, -0.6793, -0.6793, -0.6793, -0.6793,\n",
      "         -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793,\n",
      "          0.7911, -1.1546,  1.2042,  1.2042,  1.2042,  1.2042,  1.2042,  0.7911,\n",
      "         -0.6793,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,\n",
      "          0.4717,  0.4717,  0.4717,  0.4717,  0.4717]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.8428,  0.4652,  0.9453,  1.1611,  1.2799,  1.3636,  1.4278,  1.4729,\n",
      "          1.4989,  1.5098,  2.4787,  1.6029,  1.4489,  1.3266,  1.2398,  1.1847,\n",
      "          1.1361,  1.0757,  1.0085,  0.9481,  0.9049,  0.8815,  0.8663,  0.8448,\n",
      "          0.8154,  0.7859,  2.4850,  1.3888,  2.2049,  2.2337,  2.2518,  2.2571,\n",
      "          2.2496,  2.2320,  2.2062,  2.1752,  2.1450,  2.3348,  0.7697,  1.5246,\n",
      "          1.5137,  1.5121,  1.5160,  1.5178,  1.5141,  1.5085,  1.5083,  1.5159,\n",
      "          1.5262,  1.5303,  1.5225,  1.5050,  1.4879],\n",
      "        [ 1.6785,  1.6785,  1.6785,  1.6785,  1.6785,  1.6785,  1.6785,  1.6785,\n",
      "          1.6785,  1.6785,  1.6785,  1.6785, -0.8428,  0.4652,  0.9453,  1.1611,\n",
      "          1.2799,  1.3636,  2.4450,  1.5995,  1.1889,  0.9918,  0.8816,  0.8293,\n",
      "          0.7921,  0.7476,  0.6988,  0.6592,  0.6364,  0.6243,  0.6079,  0.5775,\n",
      "          2.2211,  1.2040,  2.0013,  2.0340,  2.0595,  2.0690,  2.0624,  2.3004,\n",
      "          0.6484,  1.4547,  1.4622,  1.4754,  1.4854,  1.4870,  1.4771,  1.4584,\n",
      "          1.4423,  1.4334,  1.4252,  1.4137,  1.4000]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-1.6262, -1.1546, -1.1546, -1.1546, -1.1546, -1.1546, -1.1546, -1.1546,\n",
      "         -1.1546, -1.1546,  0.7911, -0.2122, -0.6793, -0.6793, -0.6793, -0.6793,\n",
      "         -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793,\n",
      "         -0.6793, -0.6793,  0.7911, -1.1546,  1.2042,  1.2042,  1.2042,  1.2042,\n",
      "          1.2042,  1.2042,  1.2042,  1.2042,  1.2042,  0.7911, -0.6793,  0.4717,\n",
      "          0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,\n",
      "          0.4717,  0.4717,  0.4717,  0.4717,  0.4717],\n",
      "        [-1.6262, -1.1546, -1.1546, -1.1546, -1.1546, -1.1546,  0.7911, -0.2122,\n",
      "         -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793, -0.6793,\n",
      "         -0.6793, -0.6793, -0.6793, -0.6793,  0.7911, -1.1546,  1.2042,  1.2042,\n",
      "          1.2042,  1.2042,  1.2042,  0.7911, -0.6793,  0.4717,  0.4717,  0.4717,\n",
      "          0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,  0.4717,\n",
      "          0.4717,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.8428,  0.4652,  0.9453,  1.1611,  1.2799,  1.3636,  1.4278,  1.4729,\n",
      "          1.4989,  1.5098,  2.4787,  1.6029,  1.4489,  1.3266,  1.2398,  1.1847,\n",
      "          1.1361,  1.0757,  1.0085,  0.9481,  0.9049,  0.8815,  0.8663,  0.8448,\n",
      "          0.8154,  0.7859,  2.4850,  1.3888,  2.2049,  2.2337,  2.2518,  2.2571,\n",
      "          2.2496,  2.2320,  2.2062,  2.1752,  2.1450,  2.3348,  0.7697,  1.5246,\n",
      "          1.5137,  1.5121,  1.5160,  1.5178,  1.5141,  1.5085,  1.5083,  1.5159,\n",
      "          1.5262,  1.5303,  1.5225,  1.5050,  1.4879],\n",
      "        [-0.8428,  0.4652,  0.9453,  1.1611,  1.2799,  1.3636,  2.4450,  1.5995,\n",
      "          1.1889,  0.9918,  0.8816,  0.8293,  0.7921,  0.7476,  0.6988,  0.6592,\n",
      "          0.6364,  0.6243,  0.6079,  0.5775,  2.2211,  1.2040,  2.0013,  2.0340,\n",
      "          2.0595,  2.0690,  2.0624,  2.3004,  0.6484,  1.4547,  1.4622,  1.4754,\n",
      "          1.4854,  1.4870,  1.4771,  1.4584,  1.4423,  1.4334,  1.4252,  1.4137,\n",
      "          1.4000,  1.8054,  1.8389,  1.8875,  1.9359,  1.9558,  1.9494,  1.9343,\n",
      "          1.9174,  1.9085,  1.9134,  1.9243,  1.9374]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(8.2851, grad_fn=<DivBackward0>), tensor(0.))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(x, x_lens, y, y_lens, prompt_x, prompt_x_lens, prompt_y, prompt_y_lens, x_bert_feature, prompt_x_bert_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy_embedded shape: torch.Size([2, 19, 768])\n",
      "x_mask shape: torch.Size([2, 10])\n",
      "y_mask shape: torch.Size([2, 9])\n",
      "xy_mask shape: torch.Size([2, 19])\n",
      "xy_len: tensor([19, 18])\n",
      "\n",
      "xy_mask for batch idx 1:\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False])\n",
      "\n",
      "x_mask:\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n",
      "\n",
      "y_mask:\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True, False]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设的输入数据\n",
    "x = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 3, 3, 5, 5, 3, 4, 1, 67, 2]])\n",
    "y = torch.tensor(\n",
    "    [[11, 12, 13, 14, 15, 16, 17, 18, 19], [11, 12, 13, 14, 15, 3, 61, 3, 325]]\n",
    ")\n",
    "x_len = torch.tensor([10, 10])\n",
    "y_len = torch.tensor([9, 8])\n",
    "\n",
    "# 创建text和speech的Embedding层\n",
    "text_vocab_size = 250000  # 假设的文本词汇表大小\n",
    "speech_vocab_size = 250000  # 假设的语音词汇表大小\n",
    "embedding_dim = 768  # 假设的嵌入维度\n",
    "\n",
    "text_embedding = nn.Embedding(text_vocab_size, embedding_dim)\n",
    "speech_embedding = nn.Embedding(speech_vocab_size, embedding_dim)\n",
    "\n",
    "\n",
    "# 计算embedding的函数\n",
    "def compute_embedding(tokens, lengths, embedding_layer):\n",
    "    batch_size, max_len = tokens.shape\n",
    "    mask = torch.arange(max_len).expand(batch_size, max_len) < lengths.unsqueeze(1)\n",
    "    embedded = embedding_layer(tokens) * mask.unsqueeze(-1).float()\n",
    "    return embedded\n",
    "\n",
    "\n",
    "# 计算x和y的embedding\n",
    "x_embedded = compute_embedding(x, x_len, text_embedding)\n",
    "y_embedded = compute_embedding(y, y_len, speech_embedding)\n",
    "\n",
    "# 正确拼接embedding\n",
    "batch_size = x.shape[0]\n",
    "max_len = x_len.max() + y_len.max()\n",
    "xy_embedded = torch.zeros(\n",
    "    (batch_size, max_len, embedding_dim), device=x_embedded.device\n",
    ")\n",
    "\n",
    "for i in range(batch_size):\n",
    "    xy_embedded[i, : x_len[i]] = x_embedded[i, : x_len[i]]\n",
    "    xy_embedded[i, x_len[i] : x_len[i] + y_len[i]] = y_embedded[i, : y_len[i]]\n",
    "\n",
    "# 计算x_mask\n",
    "x_mask = torch.arange(x.shape[1]).expand(batch_size, -1) < x_len.unsqueeze(1)\n",
    "y_mask = torch.arange(y.shape[1]).expand(batch_size, -1) < y_len.unsqueeze(1)\n",
    "# 计算正确的xy_mask\n",
    "xy_mask = torch.zeros((batch_size, max_len), dtype=torch.bool)\n",
    "for i in range(batch_size):\n",
    "    xy_mask[i, : x_len[i] + y_len[i]] = True\n",
    "\n",
    "# 计算xy_len\n",
    "xy_len = x_len + y_len\n",
    "\n",
    "print(\"xy_embedded shape:\", xy_embedded.shape)\n",
    "print(\"x_mask shape:\", x_mask.shape)\n",
    "print(\"y_mask shape:\", y_mask.shape)\n",
    "print(\"xy_mask shape:\", xy_mask.shape)\n",
    "print(\"xy_len:\", xy_len)\n",
    "\n",
    "# 打印第二个样本（batch idx = 1）的mask\n",
    "print(\"\\nxy_mask for batch idx 1:\")\n",
    "print(xy_mask[1])\n",
    "\n",
    "# 打印x_mask\n",
    "print(\"\\nx_mask:\")\n",
    "print(x_mask)\n",
    "print(\"\\ny_mask:\")\n",
    "print(y_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "        [ 1,  3,  3,  5,  5,  3,  4,  1, 67,  2]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models import qwen2\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B\",\n",
    ")\n",
    "config.hidden_size = 768\n",
    "config.num_hidden_layers = 16\n",
    "config.max_window_layers = 16\n",
    "config.num_attention_heads = 12\n",
    "model = qwen2.Qwen2Model(config=config).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = model(inputs_embeds=xy_embedded.to(\"cuda\"), attention_mask=xy_mask.to(\"cuda\"))\n",
    "res2 = model(inputs_embeds=xy_embedded.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.last_hidden_state[0] == res2.last_hidden_state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "y_mask_int = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "y = y.type(torch.int64) * (1 - y_mask_int)\n",
    "targets = torch.nn.functional.pad(y, (0, 1), value=0) + 4096 * torch.nn.functional.pad(\n",
    "            y_mask_int, (0, 1), value=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, targets = targets[:, :-1], targets[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    2,    3,    4,    5,    6,    7,    8,    9,   10],\n",
       "        [   1,    2,    3,    4,    5,    6,    7,    8,    9, 4096]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "y_mask = torch.tensor([[True, True, True, True, True, True, True, True, True, True], [True, True, True, True, True, True, True, True, True, False]])\n",
    "EOS = 4096\n",
    "# mask为False的地方变成EOS\n",
    "y = torch.where(y_mask, y, EOS)\n",
    "# 给y后面添加EOS\n",
    "targets = torch.nn.functional.pad(y, (0, 1), value=EOS)\n",
    "y, targets = targets[:, :-1], targets[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    2,    3,    4,    5,    6,    7,    8,    9,   10],\n",
       "        [   1,    2,    3,    4,    5,    6,    7,    8,    9, 4096]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    3,    4,    5,    6,    7,    8,    9,   10, 4096],\n",
       "        [   2,    3,    4,    5,    6,    7,    8,    9, 4096, 4096]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_config = AutoConfig.from_pretrained(\n",
    "    \"Qwen/Qwen2-72b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "IGNORE_ID = -1\n",
    "text_token_len = torch.tensor([10, 10])\n",
    "speech_token_len = torch.tensor([9, 5])\n",
    "speech_token = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "lm_target = [torch.tensor([IGNORE_ID] * (2 + text_token_len[i]) + speech_token[i, :speech_token_len[i]].tolist() + [4096]) for i in range(text_token_len.size(0))]\n",
    "lm_target = pad_sequence(lm_target, batch_first=True, padding_value=IGNORE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            1,    2,    3,    4,    5,    6,    7,    8,    9, 4096],\n",
       "        [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            1,    2,    3,    4,    5, 4096,   -1,   -1,   -1,   -1]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
