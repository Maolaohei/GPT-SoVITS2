# GPT-SoVITS2

이 이름은 GPT-SoVITS의 저자 [花儿不哭](https://space.bilibili.com/5760446?spm_id_from=333.337.0.0)로부터 허가를 받았습니다.
### 이 프로젝트는 아직 개발 중이며, [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) 기반으로 개선되었습니다. 주요 개선 사항은 다음과 같습니다:
|GPT-SoVITS|GPT-SoVITS2|
|:----:|:----:|
|**텍스트**|**텍스트**|
|텍스트->전화|텍스트->BPE|
|전화->임베딩|BPE->임베딩|
|Roberta-중국어|BGE-M3|
|**음성 인코더**|**음성 인코더**|
|Hubert|$S^3$|
|VQ|$S^3$->임베딩|
|1024 음성 토큰|4096 음성 토큰|
|**AR**|**AR**|
|구식 GPT|Qwen2-0.3b|
|**음성 디코더**|**음성 디코더**|
|VITS|VITS2|
|히든 사이즈 192|히든 사이즈 256|
|2 헤드|4 헤드|
|인터 사이즈 768|인터 사이즈 1024|
|**훈련**|**훈련**|
|제로샷 훈련 없음|동일 화자로 다른 음성 추론|
|ZH,EN,JA|다국어|
|2000 시간|아직 확실하지 않음|

1. **다국어 네이티브 지원**: 중국어, 일본어, 영어에 국한되지 않고 전 세계의 모든 언어를 지원합니다.
2. **언어를 지정할 필요가 없음**: 언제든지 다국어로, 자유롭게 다국어를 혼합하여 말할 수 있습니다.
3. **다국어 텍스트 감정 추출**: 언어의 감정 분석이 더 정교해져서 말하는 방식이 더 감정적으로 풍부해집니다.
4. **Zero Shot의 향상**: 이제 모델을 미세 조정하는 것을 권장하지 않으며, 몇 초의 대상 오디오만으로 직접 제로 샷을 수행합니다.
5. **참조 오디오 융합**: 여러 참조 오디오를 업로드할 수 있으며, 결과적으로 여러 오디오를 융합한 소리를 얻을 수 있습니다.
6. **더 빠른 추론**: positional embedding을 RoPE로 변경하여, 다음 토큰을 추론할 때마다 전체 시퀀스의 embedding을 다시 계산할 필요가 없습니다.

### **데이터 및 협력 모집**: 현재 데이터를 모집하고 있습니다. QQ 1715069210, 데이터가 적합한 경우 프로젝트에 크레딧이 부여됩니다.

#### 현재 소스 코드에서 변경된 내용을 정리 중입니다. # !를 검색하면 주석을 찾을 수 있습니다. 관심이 있으시면 위의 QQ에서 교류를 희망합니다.

### 변경 목록

#### 코드북의 변경
싱글 코드북 -> 2 코드북/4 코드북
#### GPT 변경
qwen2-0.3b로 변경
#### 오디오 인코딩의 변경
cnhubert -> ~~w2v-bert-2.0(잠정적으로, 이는 meta에서 현재 훈련된 가장 큰 4.6m 시간 다국어 사전 훈련입니다. 결과가 외국인이 중국어를 말하는 것처럼 들리면 cnhubert-large로 변경)~~/cnhubert-large/mHubert-147
w2v-bert-2.0 훈련이 다소 어렵다는 것을 발견하고, mHubert-147 훈련이 비교적 쉬우며, 크기가 네 배 작고, 실제 테스트에서 fp16은 직접 충돌하며, fp32만 사용할 수 있습니다. 또한 mHubert는 이미 충분히 큽니다(600MB).
#### 텍스트 인코딩의 변경
음소 및 해당 embedding 제거
cn-roberta -> BGE-m3
#### 위치 인코딩의 변경
텍스트와 음성 인코딩을 각각 sinusoidal -> 전체를 RoPE embedding으로 변경.
#### xy 결합 embedding의 변경(실험적)
기존의
x1+x2+y1 -> y2
를
x1+y1+x2 -> y2
로 변경하고, 시퀀스 전체가 하나의 RoPE embedding을 공유
이론적으로는 이렇게 하는 것이 더 많은 대상 오디오를 확장하여 음성 라인을 융합하는 데 더 적합합니다.
예를 들어,
x1+y1+x2+y2+x3+y3+x4+y4+x5 -> y5
가
x1+x2+x3+x4+x5+y1+y2+y3+y4 -> y5
보다 더 자연스럽게 느껴질 수 있습니다. 엄밀히 증명할 수는 없습니다.
#### 차원의 변경
MLP(768, 512) -> ~~MLP 없이 직접 1024 차원. w2v-bert-2.0과 bge-m3가 모두 1024 차원이므로 완벽한 조합~~ MLP(1024, 768)
#### 훈련 방법의 변경
순수 자회귀 -> 자회귀 + 동일 스피커의 제로 샷 훈련 샘플 회귀
#### vits 변경
차원을 확장하는 방법을 모색합니다.(256 -> 512) VITS -> VITS2 (주로 플로우 모델에 transformer block 추가)
#### 형식
통일 ~~반 정밀도~~ 단 정밀도(실제 테스트 결과 반 정밀도에서는 충돌), hubert 16000 샘플링 vits 32000 샘플링 모든 오디오의 음량을 통일
#### 요약
사실 전반적으로 보면, 변경 사항은 기본적으로
1. 더 발전된 사전 훈련 모델을 사용
2. 더 발전된 모델이 더 크므로 원래의 차원도 확장
3. 제로 샷을 중시하기 때문에 훈련 방법에 제로 샷 훈련을 추가
4. 원래 코드에서는 중국어에만 bert를 사용했으나, BGE m3와 같은 다국어 embedding으로 변경함으로써 전 언어를 원활하게 추출
5. 원래 싱글 코드북만 있었고 크기가 1024에 불과하여 hubert 특징 추출 가이드 능력이 부족. 더블 코드북으로 변경하여 정보량이 1024^2 = 1048576으로 증가, 4 코드북은 더욱 과장되지만 데이터가 충분하지 않으므로 단계적으로 시도
6. 원래 느린 이유 중 하나는 GPT가 매번 시퀀스 전체의 embedding과 positional embedding을 다시 계산해야 했기 때문. 그러나 RoPE로 변경하면 이 단점이 사라짐
7. 원래는 음성 라인 융합에 관심이 없었으나, 나중에 별도의 브랜치를 만들어 원래의 GPT-SoVITS에서 음성 라인 융합을 구현. 그러나 초기 설계에는 이 목표가 전혀 포함되어 있지 않았음. 花さん의 비디오에서는 GPT 부분에서 사용되는 참조 오디오에서 얻은 음성 특징과 vits에서의 참조 오디오가 다를 수 있어 음성 라인 융합이 가능하다고 언급. 그러나 내 구현에서는 두 부분 모두에 여러 오디오가 포함됨
8. 원래 합리적이지 않다고 생각한 부분을 변경함. 예를 들어, 이미 hubert를 오디오의 embedding으로 사용했는데, 왜 다시 ar_audio_embedding이 필요한가. 그리고 원래 음소가 있었기 때문에 음소에 대한 embedding이 필요했고, 여러 개의 개별 embedding이 훈련되고 있었지만 이미 bert와 hubert를 사용하고 있었음. 또한, 개별 텍스트 embedding과 오디오 embedding은 문맥의 오디오와 텍스트를 고려하지 않으므로 직접 GPT에 입력하여 attention으로 관계를 찾는 것이 더 나음

여기까지 읽으셨다면 이해하셨을 것입니다. 이 프로젝트에 참여를 환영합니다!

**QQ: 1715069210**

**微信: JunityZ**

#### 빠른 메모
오늘 많은 논문을 읽었고, VALLE2의 논문에서 많은 새로운 아이디어를 얻었습니다. 현재 ar_audio_embedding과 ar_text_embedding은 역사적인 유물이라는 중요한 사실이 있습니다.

audioLM이 처음으로 hubert+kmeans를 사용하여 토큰을 얻었지만, kmean 양자화 학습은 전체 데이터를 학습할 필요가 없으며, hubert 분포에서 직접 학습. 그래서 후속 embedding이 추가되었습니다.

그러나 vq를 사용하면 vq 자체가 학습을 이미 수행했기 때문에 추가 embedding이 필요하지 않습니다. 여기서 역사적인 문제는 항상 embedding이 추가되고 있다는 점입니다. 영향은 크지 않겠지만, 제거하면 더 합리적일 것입니다.

또한 audio lm은 hubert와 soundstream을 통해 각각 semantic과 acoustic을 사용합니다. 그러나 GPT SoVITS도 이 기능을 가지고 있으며, meltransferencoder는 acoustic을, hubert는 semantic을 얻습니다. 매우 교묘합니다.

VALLE 계열은 일반적으로 EnCodec을 사용하며, EnCodec은 오디오에서 직접 토큰을 얻기 때문에 다시 embedding을 수행해야 합니다. hubert는 출력이 embedding이므로 필요하지 않습니다.

반대로 우리는 hubert embedding을 사용하여 토큰을 얻고, EnCodec은 토큰을 얻은 후 embedding을 수행합니다.

따라서 원래의 GPTSoVITS와 이전에 참고한 AUdio LM은 EnCodec 기반 TTS의 방법을 참고한 것처럼 보이지만, 실제로는 이 둘이 다릅니다.

#### TODO
양자화를 다시 작성하고, vector-quantize-pytorch의 Group Residual VQ를 직접 호출
